[["index.html", "Introduction to Computational Social Science Welcome", " Introduction to Computational Social Science Mark Hoffman Welcome This seminar is intended as a theoretical and methodological introduction to computational social science. Each week covers substantive and theoretical material and is associated with a technical lab. You will need to bring your laptops to each class. In the technical labs you will learn how to analyze network data in R. This e-book contains all of the technical labs in the order that we cover them. Should you forget anything we learned, you will be able to return to this e-book to cover the material again on your own. "],["reading-list.html", "1 Reading List 1.1 Readings and Software 1.2 Course Outline 1.3 Week 1 (beg. September 20th): Introductions (Lab: Introduction to R) 1.4 Week 2 (beg. September 27th): Ethics (Lab: Surveys and Survey Experiments) 1.5 Week 3 (beg. October 4th): Inequality (Lab: Collecting Data Online) 1.6 Week 4 (beg. October 11th): Polarization (Lab: Analyzing Text) 1.7 Week 5 (beg. October 18th): Markets (Lab: Regression) 1.8 Week 6 (beg. October 25th): Discrimination (Lab: Machine Learning) 1.9 Week 7 (beg. November 1st): Homophily and Diffusion (Lab: Network Analysis) 1.10 Week 8 (beg. November 8th): Semantic Change and Historical Meaning (Lab: Semantic Network Analysis) 1.11 Week 9 (beg. November 15th): Health (No Lab) 1.12 Week 10 (beg. November 29th): Group Presentations 1.13 December 11th: Final papers are due.", " 1 Reading List 1.1 Readings and Software There is one required book for the class. Salganik, M. (2019). Bit by bit: Social research in the digital age. Princeton University Press. It is a great reference for aspiring computational social scientists, so I recommend you purchase it. That said, I recognize that purchasing books can be a financial hardship, so you can also find a free copy online: https://www.bitbybitbook.com/en/1st-ed/preface/ (Links to an external site.) (Links to an external site.). All other readings can be found in book chapters or journal articles. For many of these, you will be able to download the text from JSTOR (www.JSTOR.org) or another electronic full-text service. Photocopies or PDFs of assigned readings will be made available on Canvas in the Modules section. Students are expected to bring a computer to lab. We will be using R and RStudio for our analyses. These are free computer software that you can download from https://cran.r-project.org (Links to an external site.) and https://www.rstudio.com (Links to an external site.), respectively. 1.2 Course Outline The class is divided into sections that run for one week. From the start we spend time in class in discussion (this is not a lecture class!), in exercises to computational social science methods, and in field observation and study, to collect social science data. 1.3 Week 1 (beg. September 20th): Introductions (Lab: Introduction to R) For Monday: No assignments or readings. Come prepared to say something about yourself and why you are taking this class. For Wednesday: * Our lab will be a crash course on analyzing data with R * DUE: Read Bit by Bit, Chapter 1: Introduction * DUE: Before class, install R and Rstudio, and work through this introductory tutorial on Datacamp: https://campus.datacamp.com/courses/free-introduction-to-r (Links to an external site.) 1.4 Week 2 (beg. September 27th): Ethics (Lab: Surveys and Survey Experiments) For Monday: * Kramer, A. et al. (2014). Experimental evidence of massive-scale emotional contagion through social networks. PNAS. * Kiviat, B. (2019). The Moral Limits of Predictive Practices: The Case of Credit-Based Insurance Scores. American Sociological Review, 0003122419884917. Optional Reading: * Bit by Bit, Chapter 6: Ethics For Wednesday: * Our lab will be on surveys and survey experiments. * DUE: Complete Lab Exercise 1 by the beginning of class and upload it to Canvas Optional Reading: * Bit by Bit, Chapter 3: Asking Questions 1.5 Week 3 (beg. October 4th): Inequality (Lab: Collecting Data Online) For Monday: * Salganik et al. (2006). Experimental study of inequality and unpredictability in an artificial cultural market. Science. * Adams, J., Brückner, H., &amp; Naslund, C. (2019). Who Counts as a Notable Sociologist on Wikipedia? Gender, Race, and the “Professor Test”. Socius, 5, 2378023118823946. For Wednesday: * Our lab will be about scraping data and APIs. * DUE: Complete Lab Exercise 2 by the beginning of class and upload it to Canvas Optional Readings: Bit by Bit, Chapters 2: Observing Behavior 1.6 Week 4 (beg. October 11th): Polarization (Lab: Analyzing Text) For Monday: * Hoffman, M. A. (2019). The Materiality of Ideology: Cultural Consumption and Political Thought after the American Revolution. American Journal of Sociology, 125(1), 1-62. * Bail, C. A., et al. (2018). Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences, 115(37), 9216-9221. For Wednesday: * Our lab will be about automated text analysis. * DUE: Complete Lab Exercise 3 by the beginning of class and upload it to Canvas * We will sort into groups this week for the final project. Submit your group’s research question and the names of group members to me by Wednesday of next week. 1.7 Week 5 (beg. October 18th): Markets (Lab: Regression) For Monday: * Farber, H. (2015). Why you Can’t Find a Taxi in the Rain and Other Labor Supply Lessons from Cab Drivers. Quarterly Journal of Economics. * Fourcade, M. and Healy, K. (2017.) Seeing Like a Market. Socio-Economic Review, 15:9-29. For Wednesday: * Our lab will be on regression analysis. * DUE: Submit brief (roughly 1 page) group analysis plan detailing the data you plan to use and your division of labor * DUE: Complete and email me Lab Exercise 4 by the beginning of class 1.8 Week 6 (beg. October 25th): Discrimination (Lab: Machine Learning) For Monday: * Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644. * Doleac, J. L., &amp; Stein, L. C. (2013). The visible hand: Race and online market outcomes.The Economic Journal, 123(572), F469-F492. * Karen Hao. (Jan. 21, 2019). AI is sending people to jail, and getting it wrong. MIT Technology Review: https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/ For Wednesday: * Our lab will be on machine learning using random forests and neural networks. * DUE: Complete and email me Lab Exercise 5 by the beginning of class 1.9 Week 7 (beg. November 1st): Homophily and Diffusion (Lab: Network Analysis) For Monday: * Centola, D. (2010). The spread of behavior in an online social network experiment. Science, 329(5996), 1194-1197. * Wimmer, A., &amp; Lewis, K. (2010). Beyond and below racial homophily: ERG models of a friendship network documented on Facebook. American Journal of Sociology, 116(2), 583-642. For Wednesday: * Our lab will be on network analysis. * DUE: Complete and email me Lab Exercise 6 by the beginning of class 1.10 Week 8 (beg. November 8th): Semantic Change and Historical Meaning (Lab: Semantic Network Analysis) For Monday: * Rule, A., Cointet, J. P., &amp; Bearman, P. S. (2015). Lexical shifts, substantive changes, and continuity in State of the Union discourse, 1790–2014. Proceedings of the National Academy of Sciences, 112(35), 10837-10844. * Murdock, J., Allen, C., &amp; DeDeo, S. (2017). Exploration and exploitation of Victorian science in Darwin’s reading notebooks.Cognition, 159, 117-126. Optional Reading: * Evans, James A. and Pedro Aceves (2016). “Machine Translation: Mining Text for Social Theory.” Annual Review of Sociology. For Wednesday: * Our lab will be on semantic network analysis. * No lab exercise due this week. 1.11 Week 9 (beg. November 15th): Health (No Lab) For Monday: * King, M. D., &amp; Bearman, P. S. (2011). Socioeconomic status and the increased prevalence of autism in California. American sociological review, 76(2), 320-346. * Eichstaedt, J.C., Smith, R.J., Merchant, R.M., et al. Facebook language predicts depression in medical records. Proceedings of the National Academy of Sciences, 115(44), 11203-11208. For Wednesday: * Work on your group project * No lab exercise due this week and no lab either. * Wednesday’s class will be dedicated to helping you with your projects (think of it as additional office hours/R help sessions) 1.12 Week 10 (beg. November 29th): Group Presentations 1.13 December 11th: Final papers are due. "],["installing-r-and-rstudio.html", "2 Installing R and RStudio 2.1 Downloading and Installing R 2.2 Downloading and Installing RStudio", " 2 Installing R and RStudio In this section, we will learn how to install R and RStudio. Both are freely available online. 2.1 Downloading and Installing R Description R is an open source programming language designed for statistical computing and visualization. Scientists and data analysts worldwide use it for purposes ranging from regression analysis, to natural language processing, to biological simulation, to social network analysis - the topic of this class. Being open source, users from around the world add new functions to its repositories on a daily basis. This means that the possible tools you can use and analyses you can perform with R are expanding constantly, making it an increasingly powerful environment for statistical analysis. We will show you just a glimpse of this power, but hopefully we can provide enough of a basis for you to go out on your own and learn more. Steps Navigate to https://www.r-project.org/ Click on the blue, bolded “download R” in the first paragraph. Choose a mirror (in other words, a website hosting current and past R distributions) located somewhere near to you. I am based in New York so I chose one based out of Carnegie Mellon in Pennsylvania. Download R by clicking on one of the “Download R for” links. Choose the link that accords with your operating system. I am using Mac, so I clicked: “Download R for (Mac) OS X” Next click the first link underneath the “Files” heading. This should begin the download. Follow the instructions on the installer that begins when you click on the downloaded file. Once you are finished, R should be installed on your system. 2.2 Downloading and Installing RStudio Description Next we need to install RStudio. RStudio is a user interface for R, which greatly improves the experience of working in R. As stated on its website, some of its features include: Customizable workbench with all of the tools required to work with R in one place (console, source, plots, workspace, help, history, etc.). Syntax highlighting editor with code completion. Execute code directly from the source editor (line, selection, or file). Full support for authoring Sweave and TeX documents. Runs on all major platforms (Windows, Mac, and Linux) and can also be run as a server, enabling multiple users to access the RStudio IDE using a web browser. Steps To download RStudio, navigate to https://www.rstudio.com/ and click the download RStudio button: Scroll down and click on the green “Download” button in the RStudio Desktop column. RStudio is free! This should cause your browser to scroll down to the bottom of the page where you will see a series of blue installers. Click the installer (not Zip/Tarball!) according to your operating system. This should prompt a download. Double click on the downloaded file, which will begin the installation process. If you are on Mac OS X, drag RStudio to your Applications folder. Now you are all set to go for the tutorial! Find RStudio wherever you saved it (Applications folder if you are on Mac), and open it. In the next chapter, we will learn what to do once it is up and running. "],["tour-rstudio-with-udacity.html", "3 Tour RStudio with Udacity", " 3 Tour RStudio with Udacity Now that RStudio is open watch the Udacity video below to learn more about the interface and how to use it. Once you finish, we will begin programming in R. "],["r-basics.html", "4 R Basics 4.1 Vectors 4.2 Loading Packages", " 4 R Basics This initial tutorial for R has two primary learning objectives. The first is to become affiliated with the R environment and the second is to learn how to extend the basic set of R functions to make it suitable for your own research purposes. The lessons we learn in this tutorial will serve as a strong basis for the those that follow, which focus on the actual analysis of data using R. Like most programming languages, R can serve as a calculator. We will use many of these basic mathematical operations when working with data. 2+2 ## [1] 4 ((2+2)*3)/6 ## [1] 2 2^2 ## [1] 4 We use the assignment operator “&lt;-” to save the results in a vector for later. four &lt;- 2+2 sixteen &lt;- (2+2)^2 If we type the name of the vector, it will return its values. four ## [1] 4 sixteen ## [1] 16 Functions in R also have names. Later on, we will learn to write our own functions. For now, we can make use of the large body of default functions that exist within R. The most basic function is print. We can use it to output text in the console. print(&quot;Hello world!&quot;) ## [1] &quot;Hello world!&quot; log() is another useful function and it has two arguments, x and base. When you call the function log() you have to specify x (the input value), while base has a default of exp(1). log82 &lt;- log(x = 8, base = 2) If you don’t specify the arguments in their correct order, you must use argument=value form or else you will get a different result. log1 &lt;- log(8, base = 2) log2 &lt;- log(x = 8, base = 2) log3 &lt;- log(8, 2) log4 &lt;- log(base = 2, x = 8) log5 &lt;- log(2, 8) The cat function concatenates the R objects and prints them. cat(log1, log2, log3, log4, log5) ## 3 3 3 3 0.3333333 As you can see, the fifth specification of the logarithm returned different results. 4.1 Vectors Vectors are the most basic object in R. They contain ordered elements of the same type. Vectors of size &gt; 1 are created using the “c” function. v &lt;- c(0,1,2,3,4,5,6,7,8,9) print(v) ## [1] 0 1 2 3 4 5 6 7 8 9 Computations on vectors are performed element-wise. v &lt;- v * 3 print(v) ## [1] 0 3 6 9 12 15 18 21 24 27 When we are working with a vector, we might to see what the fourth or fifth element is. We can use indexing to identify them. Indexing looks like this: v[4] ## [1] 9 v[5] ## [1] 12 Finally, we may wish to remove elements from a vector. We can use the subset function to do this. v &lt;- subset(v, v &gt; 15) print(v) ## [1] 18 21 24 27 We can perform this same operation with subscripts v[v &lt; 20] ## [1] 18 In effect, they both say - subset vector v so that only those elements greater (or less) than some value remain We can also use the subscript index method to change values if they meet a certain criteria v &lt;- c(1,2,3,4,5,6) v[v &gt; 2] &lt;- 10 print(v) ## [1] 1 2 10 10 10 10 4.2 Loading Packages Before we move on to datasets, rather than just vectors, let’s install some necessary packages. Packages are collections of R functions, data, and compiled code. They are built by members of the R community to add functionality to base R. Generally, if you wish you could do something with R, someone has built a package to do it already! We will use a few packages, some of which are built into R. We will need to install the others. For now, we just need to install tidyverse, which is the most complete data analysis and visualization package for R. To do so, we use the install.packages() function. # install.packages(&quot;tidyverse&quot;) The library function tells R to add the contents of the package to the current R session so that we can use it in our analyses. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.3 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() We will use the library() function every time we start a new R session. If R cannot find a function that you are sure is in a package you use, it normally means the package isn’t loaded or that you somehow misspelled the function name. "],["exploring-and-visualizing-data.html", "5 Exploring and Visualizing Data 5.1 Lab Assignment", " 5 Exploring and Visualizing Data Now, let’s actually get our hands dirty and start analyzing data. The first thing we will need to do is to pick a data set to analyze. A good candidate is the General Social Survey (GSS) - a large-scale survey that sociologists have been administering since the 1960s, meant to gauge the changing social attitudes and practices of Americans over time. It is probably sociology’s most celebrated dataset, the subject of tens of thousands of papers since its inception. It isn’t quite big data (only a couple thousand respondets each year), but it is a good starting point for our first data exercises. To download the GSS, first navigate to: https://gss.norc.org/get-the-data/stata Then, click where it says 2018, under the heading Download Individual Year Data Sets (cross-section only). For now, drag the downloaded file to your Desktop. It is a STATA file, so we will have to use a special R package, foreign, to load it into R as well as the function, read.dta, from that package. A function is basically a command which helps you do something. Later on, we will work on writing our own functions, for now we will us the functions that other people’s packages supply for us. # load in the foreign package.. it comes with R library(foreign) # now we can load in the GSS data gss &lt;- read.dta(&quot;Data/GSS2018.dta&quot;) You can use the View() or head() functions to actually view the data and see what it looks like. ## abany abdefect abfelegl abhelp1 abhelp2 abhelp3 abhelp4 abhlth ## 1 no yes &lt;NA&gt; yes yes yes yes yes ## 2 yes yes it depends no no no no yes ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes no yes yes &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; should yes yes yes yes &lt;NA&gt; ## 5 no yes &lt;NA&gt; no no no yes yes ## 6 yes yes should yes yes yes yes yes ## abinspay abmedgov1 ## 1 people should be able the government should decide ## 2 people should not be able &lt;NA&gt; ## 3 people should not be able a woman and her medical professional should decide ## 4 people should be able &lt;NA&gt; ## 5 people should not be able &lt;NA&gt; ## 6 people should be able &lt;NA&gt; If you have ever worked with Excel before, this should look pretty familiar! In R, this is called a data.frame(). It is the most common way we will use to organize data. class(gss) ## [1] &quot;data.frame&quot; A data.frame is organized into rows and columns. Each row holds the data for a single respondent, whereas each column holds the data for a single variable (or question). Let’s see how many people (rows) and variables (columns) the data have. # Check the number of rows nrow(gss) ## [1] 2348 # Check the number of columns ncol(gss) ## [1] 1064 # Or both at the same time! dim(gss) ## [1] 2348 1064 We can find this information over in the environment too. The rows are numbered, while the columns have names. But if you look at the column names, they won’t make much sense. # check the column names colnames(gss) ## [1] &quot;abany&quot; &quot;abdefect&quot; &quot;abfelegl&quot; &quot;abhelp1&quot; ## [5] &quot;abhelp2&quot; &quot;abhelp3&quot; &quot;abhelp4&quot; &quot;abhlth&quot; ## [9] &quot;abinspay&quot; &quot;abmedgov1&quot; &quot;abmedgov2&quot; &quot;abmelegl&quot; ## [13] &quot;abmoral&quot; &quot;abnomore&quot; &quot;abpoor&quot; &quot;abpoorw&quot; ## [17] &quot;abrape&quot; &quot;absingle&quot; &quot;abstate1&quot; &quot;abstate2&quot; ## [21] &quot;acqntsex&quot; &quot;actssoc&quot; &quot;adults&quot; &quot;advfront&quot; ## [25] &quot;affrmact&quot; &quot;afraidof&quot; &quot;afterlif&quot; &quot;age&quot; ## [29] &quot;aged&quot; &quot;agekdbrn&quot; &quot;ancestrs&quot; &quot;arthrtis&quot; ## [33] &quot;astrolgy&quot; &quot;astrosci&quot; &quot;atheists&quot; &quot;attend&quot; ## [37] &quot;attend12&quot; &quot;attendma&quot; &quot;attendpa&quot; &quot;away1&quot; ## [41] &quot;away11&quot; &quot;away2&quot; &quot;away3&quot; &quot;away4&quot; ## [45] &quot;away5&quot; &quot;away6&quot; &quot;away7&quot; &quot;babies&quot; ## [49] &quot;backpain&quot; &quot;ballot&quot; &quot;balneg&quot; &quot;balpos&quot; ## [53] &quot;befair&quot; &quot;betrlang&quot; &quot;bible&quot; &quot;bigbang&quot; ## [57] &quot;bigbang1&quot; &quot;bigbang2&quot; &quot;bird&quot; &quot;birdb4&quot; ## [61] &quot;born&quot; &quot;boyorgrl&quot; &quot;breakdwn&quot; &quot;buddhsts&quot; ## [65] &quot;buyesop&quot; &quot;buyvalue&quot; &quot;cantrust&quot; &quot;cappun&quot; ## [69] &quot;cat&quot; &quot;catb4&quot; &quot;charactr&quot; &quot;chemgen&quot; ## [73] &quot;childs&quot; &quot;chldidel&quot; &quot;christns&quot; &quot;churhpow&quot; ## [77] &quot;class&quot; &quot;clergvte&quot; &quot;closeto1&quot; &quot;closeto2&quot; ## [81] &quot;closeto3&quot; &quot;closeto4&quot; &quot;closeto5&quot; &quot;cntctfam&quot; ## [85] &quot;cntctfrd&quot; &quot;cntctkid&quot; &quot;cntctpar&quot; &quot;cntctsib&quot; ## [89] &quot;codeg&quot; &quot;coden&quot; &quot;coeduc&quot; &quot;coevwork&quot; ## [93] &quot;cofund&quot; &quot;cohort&quot; &quot;cohrs1&quot; &quot;cohrs2&quot; ## [97] &quot;coind10&quot; &quot;coisco08&quot; &quot;cojew&quot; &quot;colath&quot; ## [101] &quot;colcom&quot; &quot;coldeg1&quot; &quot;colhomo&quot; &quot;colmil&quot; ## [105] &quot;colmslm&quot; &quot;colrac&quot; &quot;colsci&quot; &quot;colscinm&quot; ## [109] &quot;comfort&quot; &quot;company&quot; &quot;compperf&quot; &quot;comprend&quot; ## [113] &quot;compuse&quot; &quot;compwage&quot; &quot;conarmy&quot; &quot;conbiz&quot; ## [117] &quot;conbus&quot; &quot;conchurh&quot; &quot;conclerg&quot; &quot;concong&quot; ## [121] &quot;concourt&quot; &quot;condemnd&quot; &quot;condom&quot; &quot;condrift&quot; ## [125] &quot;coneduc&quot; &quot;conf2f&quot; &quot;confed&quot; &quot;confinan&quot; ## [129] &quot;coninc&quot; &quot;conjudge&quot; &quot;conlabor&quot; &quot;conlegis&quot; ## [133] &quot;conmedic&quot; &quot;conpress&quot; &quot;conrinc&quot; &quot;conschls&quot; ## [137] &quot;consci&quot; &quot;consent&quot; &quot;contv&quot; &quot;conwkday&quot; ## [141] &quot;coocc10&quot; &quot;coop&quot; &quot;coother&quot; &quot;copres10&quot; ## [145] &quot;copres105plus&quot; &quot;corel&quot; &quot;cosei10&quot; &quot;cosei10educ&quot; ## [149] &quot;cosei10inc&quot; &quot;courts&quot; &quot;cowrkhlp&quot; &quot;cowrkint&quot; ## [153] &quot;cowrkslf&quot; &quot;cowrksta&quot; &quot;crack30&quot; &quot;dangoth1&quot; ## [157] &quot;dangoth2&quot; &quot;dangoth3&quot; &quot;dangoth4&quot; &quot;dangoth5&quot; ## [161] &quot;dangroth&quot; &quot;dangrslf&quot; &quot;dangslf1&quot; &quot;dangslf2&quot; ## [165] &quot;dangslf3&quot; &quot;dangslf4&quot; &quot;dangslf5&quot; &quot;dateintv&quot; ## [169] &quot;decmoney&quot; &quot;dectreat&quot; &quot;defpensn&quot; &quot;degree&quot; ## [173] &quot;demands&quot; &quot;denkid&quot; &quot;denom&quot; &quot;denom16&quot; ## [177] &quot;depress&quot; &quot;deptperf&quot; &quot;diabetes&quot; &quot;diagnosd&quot; ## [181] &quot;difrel&quot; &quot;dinefrds&quot; &quot;dipged&quot; &quot;discaff&quot; ## [185] &quot;discaffm&quot; &quot;discaffw&quot; &quot;disrspct&quot; &quot;divlaw&quot; ## [189] &quot;divorce&quot; &quot;dofirst&quot; &quot;dog&quot; &quot;dogb4&quot; ## [193] &quot;dwelling&quot; &quot;dwellpre&quot; &quot;dwelown&quot; &quot;dwelown16&quot; ## [197] &quot;earnrs&quot; &quot;earthsun&quot; &quot;educ&quot; &quot;egomeans&quot; ## [201] &quot;electron&quot; &quot;emailhr&quot; &quot;emailmin&quot; &quot;emoprobs&quot; ## [205] &quot;empinput&quot; &quot;emptrain&quot; &quot;endsmeet&quot; &quot;eqwlth&quot; ## [209] &quot;esop&quot; &quot;esopnot&quot; &quot;eth1&quot; &quot;eth2&quot; ## [213] &quot;eth3&quot; &quot;ethnic&quot; &quot;ethnum&quot; &quot;evcrack&quot; ## [217] &quot;evidu&quot; &quot;evolved&quot; &quot;evolved2&quot; &quot;evpaidsx&quot; ## [221] &quot;evstray&quot; &quot;evwork&quot; &quot;expdesgn&quot; &quot;exptext&quot; ## [225] &quot;extr2017&quot; &quot;extrapay&quot; &quot;extraval&quot; &quot;extrayr&quot; ## [229] &quot;fair&quot; &quot;fairearn&quot; &quot;famdif16&quot; &quot;famgen&quot; ## [233] &quot;family16&quot; &quot;fammhneg&quot; &quot;fampress&quot; &quot;famvswk&quot; ## [237] &quot;famwkoff&quot; &quot;fatalism&quot; &quot;fatigue&quot; &quot;fear&quot; ## [241] &quot;fechld&quot; &quot;feelevel&quot; &quot;feelrel&quot; &quot;feeused&quot; ## [245] &quot;fefam&quot; &quot;fehire&quot; &quot;fejobaff&quot; &quot;fepol&quot; ## [249] &quot;fepresch&quot; &quot;finalter&quot; &quot;finrela&quot; &quot;firstyou&quot; ## [253] &quot;fish&quot; &quot;fishb4&quot; &quot;form&quot; &quot;formwt&quot; ## [257] &quot;fringeok&quot; &quot;frndsex&quot; &quot;fucitzn&quot; &quot;fund&quot; ## [261] &quot;fund16&quot; &quot;gender1&quot; &quot;gender10&quot; &quot;gender11&quot; ## [265] &quot;gender12&quot; &quot;gender2&quot; &quot;gender3&quot; &quot;gender4&quot; ## [269] &quot;gender5&quot; &quot;gender6&quot; &quot;gender7&quot; &quot;gender8&quot; ## [273] &quot;gender9&quot; &quot;geneabrt2&quot; &quot;genegen&quot; &quot;genegoo2&quot; ## [277] &quot;geneself2&quot; &quot;genetics&quot; &quot;genetst1&quot; &quot;getahead&quot; ## [281] &quot;goat&quot; &quot;goatb4&quot; &quot;god&quot; &quot;godchnge&quot; ## [285] &quot;godmeans&quot; &quot;godswill&quot; &quot;goodlife&quot; &quot;goveqinc&quot; ## [289] &quot;govlazy&quot; &quot;govvsrel&quot; &quot;granborn&quot; &quot;grass&quot; ## [293] &quot;gunlaw&quot; &quot;handmove&quot; &quot;hapcohab&quot; &quot;hapmar&quot; ## [297] &quot;happy&quot; &quot;hapunhap&quot; &quot;haveinfo&quot; &quot;health&quot; ## [301] &quot;health1&quot; &quot;healthissp&quot; &quot;heaven&quot; &quot;hefinfo&quot; ## [305] &quot;height&quot; &quot;hell&quot; &quot;helpblk&quot; &quot;helpfrds&quot; ## [309] &quot;helpful&quot; &quot;helpnot&quot; &quot;helpoth&quot; &quot;helppoor&quot; ## [313] &quot;helpsick&quot; &quot;hhrace&quot; &quot;hhtype&quot; &quot;hhtype1&quot; ## [317] &quot;hindus&quot; &quot;hispanic&quot; &quot;hivtest&quot; &quot;hivtest1&quot; ## [321] &quot;hivtest2&quot; &quot;hlpadvce&quot; &quot;hlpdown&quot; &quot;hlpequip&quot; ## [325] &quot;hlphome&quot; &quot;hlpjob&quot; &quot;hlploan&quot; &quot;hlppaper&quot; ## [329] &quot;hlpresde&quot; &quot;hlpsick&quot; &quot;hlpsickr&quot; &quot;hlpsococ&quot; ## [333] &quot;hlthdays&quot; &quot;hlthmntl&quot; &quot;hlthphys&quot; &quot;hlthstrt&quot; ## [337] &quot;homosex&quot; &quot;homosex1&quot; &quot;hompop&quot; &quot;horse&quot; ## [341] &quot;horseb4&quot; &quot;hotcore&quot; &quot;hrs1&quot; &quot;hrs2&quot; ## [345] &quot;hrsrelax&quot; &quot;hsbio&quot; &quot;hschem&quot; &quot;hsmath&quot; ## [349] &quot;hsphys&quot; &quot;huadd&quot; &quot;huaddwhy&quot; &quot;hubbywrk&quot; ## [353] &quot;huclean&quot; &quot;hunt&quot; &quot;hunt1&quot; &quot;hurtatwk&quot; ## [357] &quot;hurtoth&quot; &quot;hurtself&quot; &quot;hvylift&quot; &quot;hyperten&quot; ## [361] &quot;id&quot; &quot;idu30&quot; &quot;if12who&quot; &quot;if16who&quot; ## [365] &quot;imbalnce&quot; &quot;imprvown&quot; &quot;imprvtrt&quot; &quot;incgap&quot; ## [369] &quot;incom16&quot; &quot;income&quot; &quot;income16&quot; &quot;incuspop&quot; ## [373] &quot;indperf&quot; &quot;indus10&quot; &quot;indusgen&quot; &quot;intage&quot; ## [377] &quot;intcntct&quot; &quot;intecon&quot; &quot;inteduc&quot; &quot;intenvir&quot; ## [381] &quot;intethn&quot; &quot;intfarm&quot; &quot;inthisp&quot; &quot;intid&quot; ## [385] &quot;intintl&quot; &quot;intlblks&quot; &quot;intlhsps&quot; &quot;intlwhts&quot; ## [389] &quot;intmed&quot; &quot;intmil&quot; &quot;intrace1&quot; &quot;intrace2&quot; ## [393] &quot;intrace3&quot; &quot;intsci&quot; &quot;intsex&quot; &quot;intspace&quot; ## [397] &quot;inttech&quot; &quot;intyrs&quot; &quot;isco08&quot; &quot;isco88&quot; ## [401] &quot;issp&quot; &quot;jew&quot; &quot;jew16&quot; &quot;jews&quot; ## [405] &quot;jobfind&quot; &quot;jobfind1&quot; &quot;joblose&quot; &quot;jobsecok&quot; ## [409] &quot;kidpars&quot; &quot;kidsinhh&quot; &quot;kidssol&quot; &quot;knowschd&quot; ## [413] &quot;knowwhat&quot; &quot;knwbus&quot; &quot;knwclenr&quot; &quot;knwcop&quot; ## [417] &quot;knwcuttr&quot; &quot;knwexec&quot; &quot;knwhrman&quot; &quot;knwlawyr&quot; ## [421] &quot;knwmchnc&quot; &quot;knwmw1&quot; &quot;knwmw2&quot; &quot;knwmw3&quot; ## [425] &quot;knwmw4&quot; &quot;knwmw5&quot; &quot;knwnurse&quot; &quot;knwtcher&quot; ## [429] &quot;laidoff&quot; &quot;lasers&quot; &quot;learnnew&quot; &quot;letdie1&quot; ## [433] &quot;letin1a&quot; &quot;libath&quot; &quot;libcom&quot; &quot;libhomo&quot; ## [437] &quot;libmil&quot; &quot;libmslm&quot; &quot;librac&quot; &quot;life&quot; ## [441] &quot;lifein5&quot; &quot;lifenow&quot; &quot;liveblks&quot; &quot;lngthinv&quot; ## [445] &quot;localnum&quot; &quot;lonely1&quot; &quot;lonely2&quot; &quot;lonely3&quot; ## [449] &quot;madeg&quot; &quot;madenkid&quot; &quot;maeduc&quot; &quot;maind10&quot; ## [453] &quot;maisco08&quot; &quot;maisco88&quot; &quot;major1&quot; &quot;major2&quot; ## [457] &quot;majorcol&quot; &quot;makefrnd&quot; &quot;maleornt&quot; &quot;manvsemp&quot; ## [461] &quot;maocc10&quot; &quot;mapres10&quot; &quot;mapres105plus&quot; &quot;mar1&quot; ## [465] &quot;mar11&quot; &quot;mar12&quot; &quot;mar2&quot; &quot;mar3&quot; ## [469] &quot;mar4&quot; &quot;mar5&quot; &quot;mar6&quot; &quot;mar7&quot; ## [473] &quot;mar8&quot; &quot;mar9&quot; &quot;marasian&quot; &quot;marblk&quot; ## [477] &quot;marcohab&quot; &quot;marelkid&quot; &quot;marhisp&quot; &quot;marhomo&quot; ## [481] &quot;marital&quot; &quot;martype&quot; &quot;marwht&quot; &quot;masei10&quot; ## [485] &quot;masei10educ&quot; &quot;masei10inc&quot; &quot;matesex&quot; &quot;mawrkgrw&quot; ## [489] &quot;mawrkslf&quot; &quot;mcsds1&quot; &quot;mcsds2&quot; &quot;mcsds3&quot; ## [493] &quot;mcsds4&quot; &quot;mcsds5&quot; &quot;mcsds6&quot; &quot;mcsds7&quot; ## [497] &quot;meddoc&quot; &quot;mentldoc&quot; &quot;mentlhos&quot; &quot;mentlill&quot; ## [501] &quot;mentloth&quot; &quot;meovrwrk&quot; &quot;mhdiagno&quot; &quot;mhp1r1&quot; ## [505] &quot;mhp1r2&quot; &quot;mhp2r1&quot; &quot;mhp2r2&quot; &quot;mhp3r1&quot; ## [509] &quot;mhp3r2&quot; &quot;mhp4r1&quot; &quot;mhp4r2&quot; &quot;mhp5r1&quot; ## [513] &quot;mhp5r2&quot; &quot;mhtreat1&quot; &quot;mhtreat2&quot; &quot;mhtreat3&quot; ## [517] &quot;mhtreat4&quot; &quot;mhtreat5&quot; &quot;mhtreatd&quot; &quot;mhunsure&quot; ## [521] &quot;miracles&quot; &quot;misswork&quot; &quot;mnthsusa&quot; &quot;mntlhlth&quot; ## [525] &quot;mobile16&quot; &quot;mode&quot; &quot;moredays&quot; &quot;muslims&quot; ## [529] &quot;mustdoc&quot; &quot;musthosp&quot; &quot;mustmed&quot; &quot;mustwork&quot; ## [533] &quot;mygoals&quot; &quot;myprobs1&quot; &quot;myprobs2&quot; &quot;myprobs3&quot; ## [537] &quot;myprobs4&quot; &quot;myprobs5&quot; &quot;myskills&quot; &quot;mywaygod&quot; ## [541] &quot;nanoben&quot; &quot;nanoharm&quot; &quot;nanowill&quot; &quot;nataccess&quot; ## [545] &quot;natactive&quot; &quot;nataid&quot; &quot;nataidy&quot; &quot;natarms&quot; ## [549] &quot;natarmsy&quot; &quot;natchld&quot; &quot;natcity&quot; &quot;natcityy&quot; ## [553] &quot;natcrime&quot; &quot;natcrimy&quot; &quot;natdrug&quot; &quot;natdrugy&quot; ## [557] &quot;nateduc&quot; &quot;nateducy&quot; &quot;natenrgy&quot; &quot;natenvir&quot; ## [561] &quot;natenviy&quot; &quot;natfare&quot; &quot;natfarey&quot; &quot;natheal&quot; ## [565] &quot;nathealy&quot; &quot;natlack&quot; &quot;natmass&quot; &quot;natmeet&quot; ## [569] &quot;natnotice&quot; &quot;natpark&quot; &quot;natrace&quot; &quot;natracey&quot; ## [573] &quot;natrelax&quot; &quot;natroad&quot; &quot;natsat&quot; &quot;natsci&quot; ## [577] &quot;natsoc&quot; &quot;natspac&quot; &quot;natspacy&quot; &quot;nattime&quot; ## [581] &quot;nattimeok&quot; &quot;natviews&quot; &quot;neisafe&quot; &quot;newfrds&quot; ## [585] &quot;news&quot; &quot;newsfrom&quot; &quot;nextgen&quot; &quot;nihilism&quot; ## [589] &quot;notsmart&quot; &quot;ntwkhard&quot; &quot;nukegen&quot; &quot;numcong&quot; ## [593] &quot;numemps&quot; &quot;numlangs&quot; &quot;nummen&quot; &quot;numorg&quot; ## [597] &quot;numpets&quot; &quot;numwomen&quot; &quot;obey&quot; &quot;occ10&quot; ## [601] &quot;odds1&quot; &quot;odds2&quot; &quot;old1&quot; &quot;old10&quot; ## [605] &quot;old11&quot; &quot;old12&quot; &quot;old2&quot; &quot;old3&quot; ## [609] &quot;old4&quot; &quot;old5&quot; &quot;old6&quot; &quot;old7&quot; ## [613] &quot;old8&quot; &quot;old9&quot; &quot;opdevel&quot; &quot;otcmed&quot; ## [617] &quot;oth16&quot; &quot;other&quot; &quot;othersex&quot; &quot;othlang&quot; ## [621] &quot;othlang1&quot; &quot;othlang2&quot; &quot;othmhneg&quot; &quot;othpet&quot; ## [625] &quot;othpetb4&quot; &quot;oversamp&quot; &quot;overwork&quot; &quot;owngun&quot; ## [629] &quot;ownstock&quot; &quot;padeg&quot; &quot;padenkid&quot; &quot;paeduc&quot; ## [633] &quot;paidsex&quot; &quot;painarms&quot; &quot;paind10&quot; &quot;paisco08&quot; ## [637] &quot;paisco88&quot; &quot;paocc10&quot; &quot;papres10&quot; &quot;papres105plus&quot; ## [641] &quot;parborn&quot; &quot;parelkid&quot; &quot;parsol&quot; &quot;partfull&quot; ## [645] &quot;partlsc&quot; &quot;partners&quot; &quot;partnrs5&quot; &quot;partpart&quot; ## [649] &quot;partteam&quot; &quot;partvol&quot; &quot;partyid&quot; &quot;pasei10&quot; ## [653] &quot;pasei10educ&quot; &quot;pasei10inc&quot; &quot;pawrkslf&quot; &quot;petb4&quot; ## [657] &quot;petb4cmfrt&quot; &quot;petb4fam&quot; &quot;petb4ply&quot; &quot;petcmfrt&quot; ## [661] &quot;petfam&quot; &quot;petplay&quot; &quot;phase&quot; &quot;phone&quot; ## [665] &quot;phyeffrt&quot; &quot;physacts&quot; &quot;physhlth&quot; &quot;physill&quot; ## [669] &quot;pig&quot; &quot;pigb4&quot; &quot;pikupsex&quot; &quot;pilingup&quot; ## [673] &quot;pillok&quot; &quot;pistol&quot; &quot;polabuse&quot; &quot;polattak&quot; ## [677] &quot;poleff11&quot; &quot;polescap&quot; &quot;polhitok&quot; &quot;polmurdr&quot; ## [681] &quot;polviews&quot; &quot;poorserv&quot; &quot;popespks&quot; &quot;popular&quot; ## [685] &quot;pornlaw&quot; &quot;posslq&quot; &quot;posslqy&quot; &quot;postlife&quot; ## [689] &quot;pray&quot; &quot;prayer&quot; &quot;prayfreq&quot; &quot;premarsx&quot; ## [693] &quot;pres12&quot; &quot;pres16&quot; &quot;prestg10&quot; &quot;prestg105plus&quot; ## [697] &quot;preteen&quot; &quot;prodctiv&quot; &quot;promtefr&quot; &quot;promteok&quot; ## [701] &quot;proudemp&quot; &quot;prvdhlth&quot; &quot;prvdold&quot; &quot;quallife&quot; ## [705] &quot;racdif1&quot; &quot;racdif2&quot; &quot;racdif3&quot; &quot;racdif4&quot; ## [709] &quot;race&quot; &quot;racecen1&quot; &quot;racecen2&quot; &quot;racecen3&quot; ## [713] &quot;raclive&quot; &quot;racopen&quot; &quot;racwork&quot; &quot;radioact&quot; ## [717] &quot;random&quot; &quot;rank&quot; &quot;ratepain&quot; &quot;ratetone&quot; ## [721] &quot;realinc&quot; &quot;realrinc&quot; &quot;reborn&quot; &quot;reg16&quot; ## [725] &quot;region&quot; &quot;relactiv&quot; &quot;relate1&quot; &quot;relate10&quot; ## [729] &quot;relate11&quot; &quot;relate12&quot; &quot;relate2&quot; &quot;relate3&quot; ## [733] &quot;relate4&quot; &quot;relate5&quot; &quot;relate6&quot; &quot;relate7&quot; ## [737] &quot;relate8&quot; &quot;relate9&quot; &quot;relatsex&quot; &quot;relext1&quot; ## [741] &quot;relext3&quot; &quot;relgenbar&quot; &quot;relgeneq&quot; &quot;relhh1&quot; ## [745] &quot;relhh10&quot; &quot;relhh11&quot; &quot;relhh12&quot; &quot;relhh2&quot; ## [749] &quot;relhh3&quot; &quot;relhh4&quot; &quot;relhh5&quot; &quot;relhh6&quot; ## [753] &quot;relhh7&quot; &quot;relhh8&quot; &quot;relhh9&quot; &quot;relhhd1&quot; ## [757] &quot;relhhd10&quot; &quot;relhhd11&quot; &quot;relhhd12&quot; &quot;relhhd2&quot; ## [761] &quot;relhhd3&quot; &quot;relhhd4&quot; &quot;relhhd5&quot; &quot;relhhd6&quot; ## [765] &quot;relhhd7&quot; &quot;relhhd8&quot; &quot;relhhd9&quot; &quot;relig&quot; ## [769] &quot;relig16&quot; &quot;religcon&quot; &quot;religint&quot; &quot;religkid&quot; ## [773] &quot;reliten&quot; &quot;relmarry&quot; &quot;relobjct&quot; &quot;relpast&quot; ## [777] &quot;relpersn&quot; &quot;relrlvnt&quot; &quot;relscrpt&quot; &quot;relsp1&quot; ## [781] &quot;relsp10&quot; &quot;relsp11&quot; &quot;relsp12&quot; &quot;relsp2&quot; ## [785] &quot;relsp3&quot; &quot;relsp4&quot; &quot;relsp5&quot; &quot;relsp6&quot; ## [789] &quot;relsp7&quot; &quot;relsp8&quot; &quot;relsp9&quot; &quot;relsprt&quot; ## [793] &quot;reptile&quot; &quot;reptileb4&quot; &quot;res16&quot; &quot;respect&quot; ## [797] &quot;respnum&quot; &quot;respond&quot; &quot;rfamlook&quot; &quot;rgroomed&quot; ## [801] &quot;rhlthend&quot; &quot;richwork&quot; &quot;rifle&quot; &quot;rincblls&quot; ## [805] &quot;rincom16&quot; &quot;rincome&quot; &quot;rlooks&quot; &quot;rowngun&quot; ## [809] &quot;rplace&quot; &quot;rvisitor&quot; &quot;rweight&quot; &quot;rxmed&quot; ## [813] &quot;safefrst&quot; &quot;safehlth&quot; &quot;safetywk&quot; &quot;sampcode&quot; ## [817] &quot;sample&quot; &quot;satfam7&quot; &quot;satfin&quot; &quot;satjob&quot; ## [821] &quot;satjob1&quot; &quot;satlife&quot; &quot;satsoc&quot; &quot;savesoul&quot; ## [825] &quot;scibnfts&quot; &quot;scientbe&quot; &quot;scientgo&quot; &quot;scienthe&quot; ## [829] &quot;scientod&quot; &quot;scifrom&quot; &quot;scinews1&quot; &quot;scinews2&quot; ## [833] &quot;scinews3&quot; &quot;scistudy&quot; &quot;scitext&quot; &quot;secondwk&quot; ## [837] &quot;seeksci&quot; &quot;seetalk1&quot; &quot;seetalk2&quot; &quot;seetalk3&quot; ## [841] &quot;seetalk4&quot; &quot;seetalk5&quot; &quot;sei10&quot; &quot;sei10educ&quot; ## [845] &quot;sei10inc&quot; &quot;selfhelp&quot; &quot;seriousp&quot; &quot;severe1&quot; ## [849] &quot;severe2&quot; &quot;severe3&quot; &quot;severe4&quot; &quot;severe5&quot; ## [853] &quot;sex&quot; &quot;sexbirth&quot; &quot;sexeduc&quot; &quot;sexfreq&quot; ## [857] &quot;sexnow&quot; &quot;sexornt&quot; &quot;sexsex&quot; &quot;sexsex5&quot; ## [861] &quot;shotgun&quot; &quot;sibs&quot; &quot;size&quot; &quot;slfmangd&quot; ## [865] &quot;slpprblm&quot; &quot;smallgap&quot; &quot;smammal&quot; &quot;smammalb4&quot; ## [869] &quot;socbar&quot; &quot;socfrend&quot; &quot;socommun&quot; &quot;socrel&quot; ## [873] &quot;solarrev&quot; &quot;spaneng&quot; &quot;spanint&quot; &quot;spanking&quot; ## [877] &quot;spanself&quot; &quot;spdeg&quot; &quot;spden&quot; &quot;speduc&quot; ## [881] &quot;spevwork&quot; &quot;spfalook&quot; &quot;spfund&quot; &quot;sphealer&quot; ## [885] &quot;sphrs1&quot; &quot;sphrs2&quot; &quot;spind10&quot; &quot;spisco08&quot; ## [889] &quot;spisco88&quot; &quot;spjew&quot; &quot;spkath&quot; &quot;spkcom&quot; ## [893] &quot;spkhomo&quot; &quot;spklang&quot; &quot;spkmil&quot; &quot;spkmslm&quot; ## [897] &quot;spkrac&quot; &quot;splive&quot; &quot;spocc10&quot; &quot;spother&quot; ## [901] &quot;sppres10&quot; &quot;sppres105plus&quot; &quot;sprel&quot; &quot;sprtprsn&quot; ## [905] &quot;spsei10&quot; &quot;spsei10educ&quot; &quot;spsei10inc&quot; &quot;spvtrfair&quot; ## [909] &quot;spwksup&quot; &quot;spwrkslf&quot; &quot;spwrksta&quot; &quot;srcbelt&quot; ## [913] &quot;stockops&quot; &quot;stockval&quot; &quot;stress&quot; &quot;stress12&quot; ## [917] &quot;stresses&quot; &quot;strredpg&quot; &quot;suicide1&quot; &quot;suicide2&quot; ## [921] &quot;suicide3&quot; &quot;suicide4&quot; &quot;supcares&quot; &quot;supervis&quot; ## [925] &quot;suphelp&quot; &quot;tax&quot; &quot;teamsafe&quot; &quot;teens&quot; ## [929] &quot;teensex&quot; &quot;tempgen&quot; &quot;theism&quot; &quot;thnkself&quot; ## [933] &quot;threaten&quot; &quot;tlkclrgy&quot; &quot;tlkfam&quot; &quot;toofast&quot; ## [937] &quot;toofewwk&quot; &quot;trbigbus&quot; &quot;trcourts&quot; &quot;trdunion&quot; ## [941] &quot;trust&quot; &quot;trustman&quot; &quot;trustsci&quot; &quot;trynewjb&quot; ## [945] &quot;tvhours&quot; &quot;unemp&quot; &quot;unhappy&quot; &quot;union&quot; ## [949] &quot;union1&quot; &quot;unrelat&quot; &quot;upsdowns&quot; &quot;upset&quot; ## [953] &quot;uscitzn&quot; &quot;usedup&quot; &quot;usetech&quot; &quot;usewww&quot; ## [957] &quot;uswary&quot; &quot;version&quot; &quot;vetyears&quot; &quot;vigfrnd&quot; ## [961] &quot;viggrp&quot; &quot;viglabel&quot; &quot;vigmar&quot; &quot;vignei&quot; ## [965] &quot;vigsoc&quot; &quot;vigversn&quot; &quot;vigwork&quot; &quot;viruses&quot; ## [969] &quot;visitors&quot; &quot;visnhist&quot; &quot;vissci&quot; &quot;vistholy&quot; ## [973] &quot;viszoo&quot; &quot;vote12&quot; &quot;vote16&quot; &quot;vpsu&quot; ## [977] &quot;vstrat&quot; &quot;watergen&quot; &quot;waypaid&quot; &quot;wayraise&quot; ## [981] &quot;wealth&quot; &quot;webmob&quot; &quot;weekswrk&quot; &quot;weight&quot; ## [985] &quot;where1&quot; &quot;where11&quot; &quot;where2&quot; &quot;where3&quot; ## [989] &quot;where4&quot; &quot;where5&quot; &quot;where6&quot; &quot;where7&quot; ## [993] &quot;whoelse1&quot; &quot;whoelse2&quot; &quot;whoelse3&quot; &quot;whoelse4&quot; ## [997] &quot;whoelse5&quot; &quot;whoelse6&quot; &quot;whynopet&quot; &quot;whywkhme&quot; ## [1001] &quot;widowed&quot; &quot;wkageism&quot; &quot;wkdecide&quot; &quot;wkfreedm&quot; ## [1005] &quot;wkharoth&quot; &quot;wkharsex&quot; &quot;wkpraise&quot; &quot;wkracism&quot; ## [1009] &quot;wksexism&quot; &quot;wksmooth&quot; &quot;wksub&quot; &quot;wksub1&quot; ## [1013] &quot;wksubs&quot; &quot;wksubs1&quot; &quot;wksup&quot; &quot;wksup1&quot; ## [1017] &quot;wksups&quot; &quot;wksups1&quot; &quot;wkvsfam&quot; &quot;wlthblks&quot; ## [1021] &quot;wlthhsps&quot; &quot;wlthwhts&quot; &quot;worda&quot; &quot;wordb&quot; ## [1025] &quot;wordc&quot; &quot;wordd&quot; &quot;worde&quot; &quot;wordf&quot; ## [1029] &quot;wordg&quot; &quot;wordh&quot; &quot;wordi&quot; &quot;wordj&quot; ## [1033] &quot;wordsum&quot; &quot;workblks&quot; &quot;workdiff&quot; &quot;workfast&quot; ## [1037] &quot;workfor1&quot; &quot;workhard&quot; &quot;workhsps&quot; &quot;workwhts&quot; ## [1041] &quot;wrkgovt&quot; &quot;wrkhome&quot; &quot;wrksched&quot; &quot;wrkslf&quot; ## [1045] &quot;wrkslffam&quot; &quot;wrkstat&quot; &quot;wrktime&quot; &quot;wrktype&quot; ## [1049] &quot;wrkwayup&quot; &quot;wtss&quot; &quot;wtssall&quot; &quot;wtssnr&quot; ## [1053] &quot;wwwhr&quot; &quot;wwwmin&quot; &quot;xmarsex&quot; &quot;xmarsex1&quot; ## [1057] &quot;xmovie&quot; &quot;xnorcsiz&quot; &quot;year&quot; &quot;yearsjob&quot; ## [1061] &quot;yearsusa&quot; &quot;yearval&quot; &quot;yousup&quot; &quot;zodiac&quot; They are like a secret code - if you know what the code means then you can figure out what information a variable holds. But how do we figure that out? We have to use a codebook! Head over to https://gssdataexplorer.norc.org/variables/vfilter for an accessible online codebook. You can search for keywords to see if it has variables that you care about. Here are some random suggestions: * age * income * partyid * sex * race * hrs1 We can evaluate these variables more closely using various R functions. For example, what are respondents’ average age? We use the mean function for that, and specify the na.rm = T option to tell R that respodent’s who didn’t report an age should be ignored. mean(gss$age, na.rm = T) ## [1] 48.97138 Interesting - almost 50 years old! By comparison, the average age in the US is roughly 38 years old. This might signal that the GSS is skewed towards older respondents, but we have to remember that babies do not take surveys. We can use the summary function to get a host of information about a variable’s distribution. For example, below we apply it to partyid - a measure of political affiliation which goes from 1 (Strong Demcorat) to 7 (Strong Republican) [the reason that it is different in practice than what the codebook says is because of how R converts factors to numerics, which we have to do to examine the scale quantitatively – specifically, the first option of a factor in R will be a 1 rather than a 0]. The median is a 4, which is dabsmack in the middle of the scale. The median respondent is an independent with no reported party leanings. summary(as.numeric(gss$partyid), na.rm = T) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 2.000 4.000 3.883 6.000 8.000 33 Cool! What if we wanted to see how two variables relate to one another? For example, we have heard from pundits that racial background strongly shapes political leaning in the today’s America – does the GSS confirm that conclusion? For comparison, we can also look at how people of varioius racial backgrounds differ by age. To achieve this, we first have to group the data by race using the group_by function from tidyverse. gss &lt;- gss %&gt;% group_by(race) Now that respondents are grouped by race, we can use the summarize function to evaluate each group’s average party identification and age. vars_by_race &lt;- gss %&gt;% summarise( partyid = mean(as.numeric(partyid), na.rm = T), age = mean(age, na.rm = T) ) vars_by_race ## # A tibble: 3 × 3 ## race partyid age ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 white 4.25 50.5 ## 2 black 2.45 46.0 ## 3 other 3.61 43.3 With respect to politics - just as we expected! People who identify as white also tend to be a bit older on average than people who identify as black. Instead of just looking at the averages, we could visualize them using a simple barplot. We will use the ggplot function for this, which is one of the most useful suite of functions for visualization in all of R. ggplot(vars_by_race, aes(x=race, y=partyid, fill=race)) + geom_bar(stat=&quot;identity&quot;) It is a complex function, so let’s break it down. The first argument is the data we want to visualize itself - vars_by_race. Then we need to establish the aesthetics (i.e. aes()). x is the x variable (i.e. the variable on the x-axis of the graph), y is the y variable (i.e. the varible on the y axis of the graph), and fill is the variable we want to use to color the bars (race). Now that the aesthetics are established, we tell ggplot what kind of plot we want (geom_bar specifies a bar plot). And stat = “identity” means we want to graph the values as they are in the data we provided, rather than trying to do something else with them. Barplots are useful when you want to see the relationship between a continuous variable (like partyid) and a categorical variable (like race). But what if we have two continuous variables, like age and how many hours one works a week? We can use scatterplots for that! First, let’s return the data to its original state by ungrouping it. gss &lt;- gss %&gt;% ungroup() Now we can use geom_point, instead of bar, to plot the relationship between age and hours worked last week (hrs1). ggplot(data = gss, aes(x = age, y = hrs1)) + geom_point() ## Warning: Removed 973 rows containing missing values (geom_point). With geom_smooth(), we can add a best fit line to better understand the relationship. ggplot(data = gss, aes(x = age, y = hrs1)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Warning: Removed 973 rows containing non-finite values (stat_smooth). ## Warning: Removed 973 rows containing missing values (geom_point). Pretty flat, with a bit of a decrease after age 50, and a weird uptick around 100 years old… perhaps that is just noise? Let’s filter out the really old people from the data. It is simple using the filter command. All we have to specify is that we want people of age less than 80, like so. gss_younger &lt;- gss %&gt;% filter(age &lt; 80) Now we replot the data, except we have to change the name of the data we are using to gss_younger! ggplot(data = gss_younger, aes(x = age, y = hrs1)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Warning: Removed 853 rows containing non-finite values (stat_smooth). ## Warning: Removed 853 rows containing missing values (geom_point). Great! We can add in another variable using color, if we want. Coloring by sex, for example, reveals remaining discrepencies in workforce participation between males and females, though there is a lot of overlap, and much more than we would have seen twenty years ago. ggplot(data = gss, aes(x = age, y = hrs1, color = sex)) + geom_point() ## Warning: Removed 973 rows containing missing values (geom_point). If we add geom_smooth() to this, we actually get two lines, one for each group! ggplot(data = gss, aes(x = age, y = hrs1, color = sex)) + geom_point() ## Warning: Removed 973 rows containing missing values (geom_point). Awesome! That is it for today. 5.1 Lab Assignment Use the GSS to produce an intersting or surprising visualization of your own! We will look at them in class. "],["surveys-and-survey-experiments-with-qualtrics.html", "6 Surveys and Survey Experiments with Qualtrics 6.1 Creating a Qualtrics account 6.2 Building a survey 6.3 Survey options 6.4 A quick survey experiment 6.5 Publish and Distribute", " 6 Surveys and Survey Experiments with Qualtrics In this chapter, we will go from using data that someone else collected to collecting data of our own using Qualtrics. Qualtrics makes collecting survey data online easy. If you have ever used something like Google Forms, it is, in many respects, quite similar, except that it is much more powerful. As you will see, we can quickly and easily build a survey experiment, administer the experiment, download the results, and import them into R. Let’s get started! 6.1 Creating a Qualtrics account Stanford is kind enough to provide all of us with access to Qualtrics. Navigate to https://uit.stanford.edu/service/survey to get started. Click on the big, green “Set up a Qualtrics account” button and it will tell you to go to https://stanforduniversity.qualtrics.com. Go to that link! There, it will ask if you have a preexisting Qualtrics account or not. Click the button that pertains to you. If you don’t have a preexisting account, it will either prompt you to log-in to your Stanford account via WebAuth, or else it will automatically log-in for you and set up your account if you are already logged in. Now you should be into Qualtrics! Since you haven’t made a project previously, you only have one option: Create new project. Everytime you want to make a new survey you will click this button. We will make an example survey to show you the ropes. It will bring you to a page with a bunch of options for creating a new survey. You can start from these pre-designed surveys if they fall in line with the kind of survey you want to run. There are even pre-built surveys for academics. For learning purposes though, we will start from scratch by clicking “Create your own”. Now it asks us to name our project. I called it Trial Survey. 6.2 Building a survey With a project created, we have been ported to Qualtric’s main user interface, where we can build a survey. Have a look around at all of the different options. Qualtrics is full of tools for creating your ideal survey, but at times that functionality can be a bit overwhelming. We will start simple. Let’s start by learning how to add a question to our survey. Click the button that says “Text Entry” on the far right side of the interface. It should pull up the following menu. There are a bunch of options and I recommend experimenting/reading up on all of them. The most essential are Text Entry, Multiple Choice, and Matrix Table. Our first question will use Text Entry. It allows us to ask a question and then have the user put in any value that they like. Multiple Choice on the other hand would let us specify the answers they can provide. For a question like – How old are you? – there are too many potential responses for multiple choice to work well, so text entry will do. We can change the text at the top of the question box to specify the question the respondent should answer. For example: Next let’s make a more complicated multiple choice question using the matrix table. It basically allows you to ask multiple multiple choice questions at once. For example, we can copy a question from the GSS about trust in institutions to see the extent to which our respondents trust different groups, like their family or the government. Cool! Maybe trust of this sort varies by income - we might expect rich people to support institutions more than the poor, for example, because they have historically benefitted from them. But what if they ignore the question and put in something other than their income into the question response? How do we make sure they answer in the way that we want them to? We can use validation for that. On the right hand side, below the question type, we can see a bunch of options for each question. Look for the area that says Validation Type. Click Content Validation. It will produce subsidiary options under Content Type. We can click Number to make sure that the respondents put a number in the text box! If we want to put the income question after age, we can simply drag it upwards so that it comes before the question on trust. We don’t have to come up with all of the questions ourselves - Qualtrics has a question bank that you can use to find pre-written questions. Just click on the “Import Questions from..” button at the bottom of the page. I found one in the Survey Library -&gt; Qualtrics Library -&gt; Higher Education section about the quality of your professors… how would you rate us? Cool, we have created a completely nonsensical survey, but that’s okay. 6.3 Survey options Let’s say you want to edit some meta-options about the survey and how it behaves, for example, whether respondents can go backwards to previous questions or not. Click the “Survey Options” button near the top of the page and it will pull up a bunch of selections you can make and attributes you can change about the survey. “Look and Feel” will let you change how your survey looks, though probably not how it feels. 6.4 A quick survey experiment To set up a survey experiment, we have to set up a block of questions from which respondents will only get one of the N questions in the block. This selection is randomized, so that we can see how the randomization affects their responses. Here is a quick example. First add a new block: Now we have two blocks: Now add the two different versions of the survey prompt to the block. Here I took a question from a survey about how Americans respond to hostage crises. The last step is to make sure that the questions from this block are sampled randomly for each respondent, and that only one of these questions will be selected. We can achieve this by editing the block’s options. Click block options like so and select the Question Randomization option: This should pull up the Question Randomization menu. We can choose: no randomization, to randomize the order of questions, or to randomly select N questions from the block to present to the respondent. Select the last optin and set the number to 1 and we will have our first survey experiment. 6.5 Publish and Distribute You can try out your survey by pressing the blue “Preview” button at the top. Or if you are ready, press the green “Publish” button. Once it is published, we can distribute it to respondents. We could do this on Mechanical Turk, as we will learn in a later class, or we can just send it to our friends, as I want you to do for this week’s lab. To do that click on the “Distributions” tab at the top of the survey. It will ask how you want to distribute your survey. You can do it by email, or by my preferred way, which is to “Get a single reusable link” which you can send to anyone who you want to take your survey. You can also generate a trackable link for each respondent if you want to keep track of who responded. Let’s follow the link and fill it out ourselves. Once you complete both pages, you will be sent to the completion page. Now inside the Data and Analysis pane in Qualtrics, you should see a response! We can export it using the “Export &amp; Import” data button and then clicking Export Data. Doing so will pull up this pane. Export it as a .csv because those files are super easy to load into R. It will download a .zip file to your computer. Once you unzip it, you will be able to load the resulting file into R. First, I would rename it, so that it is easy to input into R. I called the file trial_survey.csv. Drag it to your R project and then you can load it into R using the read.csv() function. Simple! survey &lt;- read.csv(&quot;Data/trial_survey.csv&quot;) Cool - what does it look like? head(survey) ## StartDate ## 1 Start Date ## 2 {&quot;ImportId&quot;:&quot;startDate&quot;,&quot;timeZone&quot;:&quot;America/Denver&quot;} ## 3 2020-01-15 13:41:21 ## EndDate Status ## 1 End Date Response Type ## 2 {&quot;ImportId&quot;:&quot;endDate&quot;,&quot;timeZone&quot;:&quot;America/Denver&quot;} {&quot;ImportId&quot;:&quot;status&quot;} ## 3 2020-01-15 13:42:45 IP Address ## IPAddress Progress Duration..in.seconds. ## 1 IP Address Progress Duration (in seconds) ## 2 {&quot;ImportId&quot;:&quot;ipAddress&quot;} {&quot;ImportId&quot;:&quot;progress&quot;} {&quot;ImportId&quot;:&quot;duration&quot;} ## 3 68.65.165.140 100 84 ## Finished ## 1 Finished ## 2 {&quot;ImportId&quot;:&quot;finished&quot;} ## 3 True ## RecordedDate ## 1 Recorded Date ## 2 {&quot;ImportId&quot;:&quot;recordedDate&quot;,&quot;timeZone&quot;:&quot;America/Denver&quot;} ## 3 2020-01-15 13:42:46 ## ResponseId RecipientLastName ## 1 Response ID Recipient Last Name ## 2 {&quot;ImportId&quot;:&quot;_recordId&quot;} {&quot;ImportId&quot;:&quot;recipientLastName&quot;} ## 3 R_79wZUWUMY18MclX ## RecipientFirstName RecipientEmail ## 1 Recipient First Name Recipient Email ## 2 {&quot;ImportId&quot;:&quot;recipientFirstName&quot;} {&quot;ImportId&quot;:&quot;recipientEmail&quot;} ## 3 ## ExternalReference LocationLatitude ## 1 External Data Reference Location Latitude ## 2 {&quot;ImportId&quot;:&quot;externalDataReference&quot;} {&quot;ImportId&quot;:&quot;locationLatitude&quot;} ## 3 37.423004150390625 ## LocationLongitude DistributionChannel ## 1 Location Longitude Distribution Channel ## 2 {&quot;ImportId&quot;:&quot;locationLongitude&quot;} {&quot;ImportId&quot;:&quot;distributionChannel&quot;} ## 3 -122.1638946533203125 anonymous ## UserLanguage Q1 ## 1 User Language How old are you? ## 2 {&quot;ImportId&quot;:&quot;userLanguage&quot;} {&quot;ImportId&quot;:&quot;QID1_TEXT&quot;} ## 3 EN 12 ## Q3 ## 1 How many dollars did you make this past year, after taxes? ## 2 {&quot;ImportId&quot;:&quot;QID3_TEXT&quot;} ## 3 20000 ## Q2_1 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Banks ## 2 {&quot;ImportId&quot;:&quot;QID2_1&quot;} ## 3 Hardly any ## Q2_2 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - The Government ## 2 {&quot;ImportId&quot;:&quot;QID2_2&quot;} ## 3 Only some ## Q2_3 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Corporations ## 2 {&quot;ImportId&quot;:&quot;QID2_3&quot;} ## 3 Hardly any ## Q2_4 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Your neighbors ## 2 {&quot;ImportId&quot;:&quot;QID2_4&quot;} ## 3 A great deal ## Q2_5 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Your family members ## 2 {&quot;ImportId&quot;:&quot;QID2_5&quot;} ## 3 A great deal ## Q2_6 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - The Media ## 2 {&quot;ImportId&quot;:&quot;QID2_6&quot;} ## 3 Only some ## Q5 ## 1 Overall, how well do the professors at this university teach? ## 2 {&quot;ImportId&quot;:&quot;QID4&quot;} ## 3 Not well at all ## Q5.1 ## 1 A terrorist group has taken 100 US citizens hostage and threatens to kill them if the US does not release several of their members currently being held at Guantanamo bay.\\nWhat should the President do? ## 2 {&quot;ImportId&quot;:&quot;QID5&quot;} ## 3 Agree to the deal ## Q6 ## 1 A terrorist group has taken 2 US citizens hostage and threatens to kill them if the US does not release several of their members currently being held at Guantanamo bay.\\nWhat should the President do? ## 2 {&quot;ImportId&quot;:&quot;QID6&quot;} ## 3 Looks great except for the second row! Let’s drop that and then you will be ready to analyze it using the tools from last class. survey &lt;- survey[-2,] head(survey) ## StartDate EndDate Status IPAddress Progress ## 1 Start Date End Date Response Type IP Address Progress ## 3 2020-01-15 13:41:21 2020-01-15 13:42:45 IP Address 68.65.165.140 100 ## Duration..in.seconds. Finished RecordedDate ResponseId ## 1 Duration (in seconds) Finished Recorded Date Response ID ## 3 84 True 2020-01-15 13:42:46 R_79wZUWUMY18MclX ## RecipientLastName RecipientFirstName RecipientEmail ## 1 Recipient Last Name Recipient First Name Recipient Email ## 3 ## ExternalReference LocationLatitude LocationLongitude ## 1 External Data Reference Location Latitude Location Longitude ## 3 37.423004150390625 -122.1638946533203125 ## DistributionChannel UserLanguage Q1 ## 1 Distribution Channel User Language How old are you? ## 3 anonymous EN 12 ## Q3 ## 1 How many dollars did you make this past year, after taxes? ## 3 20000 ## Q2_1 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Banks ## 3 Hardly any ## Q2_2 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - The Government ## 3 Only some ## Q2_3 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Corporations ## 3 Hardly any ## Q2_4 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Your neighbors ## 3 A great deal ## Q2_5 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - Your family members ## 3 A great deal ## Q2_6 ## 1 I am going to name some groups in this country. As far as the people in these groups are concerned, how much trust do you have for them? - The Media ## 3 Only some ## Q5 ## 1 Overall, how well do the professors at this university teach? ## 3 Not well at all ## Q5.1 ## 1 A terrorist group has taken 100 US citizens hostage and threatens to kill them if the US does not release several of their members currently being held at Guantanamo bay.\\nWhat should the President do? ## 3 Agree to the deal ## Q6 ## 1 A terrorist group has taken 2 US citizens hostage and threatens to kill them if the US does not release several of their members currently being held at Guantanamo bay.\\nWhat should the President do? ## 3 "],["collecting-data-online.html", "7 Collecting data online 7.1 Scraping the web", " 7 Collecting data online In previous tutorials, we learned how to download datasets online or collect them ourselves using survey software like Qualtrics and to load them into R for analysis. A lot of computational social science data, however, doesn’t come in such easily downloadable form. This is for a couple reasons. For one, companies might provide their data for viewing on a webpage rather than in a .csv or .dta file. Wikipedia, for example, contains thousands and thousands of data tables, concerning topics from GDP per capita by country to the number and types of awards won by Kanye West. These data are publicly available in the form of tables on their various webpages; but there is no single place where you can download all of them. We therefore will need to learn how to grab this data off their webpages using something called web scraping or crawling in R. This is, in part, what Google uses to index the content of websites and offer them to you following your searches. Alternatively, the amount of data that a website provides might just be too large or too heterogenous to reasonably put into a single, downloadable file or else webistes might have privacy and intellectual property concerns, which preclude them from making all of their data easily available to everyone. In such situations, a webiste or organization might provide a tool for accessing their data in an approved fashion, often referred to as an application programming interface (API). In tutorial, we will learn the aforementioned skills for collecting online data - web scraping and using APIs. Each website and API is different, so the tutorials presented here might not always apply precisely to every use case, but the basic principles should help get you started collecting digital trace data. 7.1 Scraping the web In the first portion of this tutorial, we will cover web scraping. Web scraping involves pulling the underlying code – HTML, CSS, or Javascript – of website and interpreting or collecting information embedded in that code. When you visit a website, your browser reads the HTML, CSS, and Javascript and through interpreting them, learns how to display that website. HTML defines the meaning and structure of web content, while CSS (Cascading Style Sheets) and Javascript in turn define how that content appears and behaves, respectively. One of the best packages in R for webscraping is rvest. In particular, it focuses on pulling data from html web pages and has a number of functions for doing so. You could build a program which grabs the HTML from websites and searches through it for information, but it would be very difficult. rvest has done the difficult work for you. Let’s begin by installing rvest. install.packages(&quot;rvest&quot;) Now we can load it into R. Let’s load in tidyverse too. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) Great! If you want to read more about rvest beyond what is covered here, check out its reference manual online: https://cran.r-project.org/web/packages/rvest/rvest.pdf The core function in rvest for grabbing the html data from a website is html(). We will use it to grab data from Wikipedia about the GDP per person employed. We could grab any webpage using this function and it is worth exploring on your own time. webpage &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_person_employed&quot;) webpage ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... Cool! With the HTML in our R environment, we can now use other functions to extract information from it. How do we do that? Well first, we have to know what function to use. There are quite a few - for example, html_attr extracts text or tag names, html_nodes extracts sections or divisions of the html file by name or type, html_table extracts tables inside of sections, and html_text extracts text. Once we know the function that we need to use, then we have to figure out what we want to pull from the html. Go to our URL: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_person_employed Find the table to extract. Right-click the table -&gt; click Inspect On the right-hand side, a pop-up menu will show and you will need to select the table element Right-click the table element -&gt; Copy -&gt; Copy Xpath We can then use html_nodes with the xpath argument set to the copied Xpath to extract just the html information for the table of interest. webpage_table_html &lt;-html_nodes(webpage, xpath=&#39;//*[@id=&quot;mw-content-text&quot;]/div[1]/table/tbody/tr[2]/td[1]/table&#39;) webpage_table_html ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable sortable&quot; style=&quot;margin-left:auto;margin-right:au ... The problem is that the table is still in html format - thankfully, rvest has a function, html_table, which converts such information into an R data.frame. It saves each table in the html as a data.frame inside of a single list, so we will have to index the list we want. In this case, there is only one table on the page so we will index the first item of the list. gdp_info &lt;- html_table(webpage_table_html, fill = T, trim = T) class(gdp_info) ## [1] &quot;list&quot; gdp_info &lt;- gdp_info[[1]] There isn’t much we can do with a single column of data like this. So what if we scraped data about countries from another Wikipedia page and merged it to this one? For example, we could evaluate if GDP per hour worked (which implicitly adjusts for country size and captures hourly returns to labor) is correlated with how a country performed at the 2016 Olympic games. First let’s grab the info just like we did before. # Grab the html olympics_webpage &lt;- read_html(&quot;https://en.wikipedia.org/wiki/2016_Summer_Olympics_medal_table&quot;) # Extract the table olympics_webpage_table_html &lt;-html_nodes(olympics_webpage, xpath=&#39;//*[@id=&quot;mw-content-text&quot;]/div[1]/table[3]&#39;) # Convert the table to a data.frame medals_info &lt;- html_table(olympics_webpage_table_html, fill = T, trim = T) medals_info &lt;- medals_info[[1]] Then let’s inspect the data. Does it look like it can be easily merged with our GDP data? Perhaps, but there is one problem. The country names on the Olympics page have abbreviations next to them.. this will probably trip up R. head(medals_info) ## # A tibble: 6 × 6 ## Rank NOC Gold Silver Bronze Total ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 United States (USA) 46 37 38 121 ## 2 2 Great Britain (GBR) 27 23 17 67 ## 3 3 China (CHN) 26 18 26 70 ## 4 4 Russia (RUS) 19 17 20 56 ## 5 5 Germany (GER) 17 10 15 42 ## 6 6 Japan (JPN) 12 8 21 41 One option is to try and remove those parentheses from the country names. A strategy for doing that is to split each country name into each of its respective characters, identify which of the characters is a (forward facing?) parenthesis, and to keep only the set of characters which come before that parenthesis. Then we paste all of our country names back together. To split strings, we can use the strsplit() function which comes with base R. split_countries &lt;- strsplit(medals_info$NOC, &quot;&quot;) head(split_countries) ## [[1]] ## [1] &quot;U&quot; &quot;n&quot; &quot;i&quot; &quot;t&quot; &quot;e&quot; &quot;d&quot; &quot; &quot; &quot;S&quot; &quot;t&quot; &quot;a&quot; &quot;t&quot; &quot;e&quot; &quot;s&quot; &quot; &quot; &quot;(&quot; &quot;U&quot; &quot;S&quot; &quot;A&quot; &quot;)&quot; ## ## [[2]] ## [1] &quot;G&quot; &quot;r&quot; &quot;e&quot; &quot;a&quot; &quot;t&quot; &quot; &quot; &quot;B&quot; &quot;r&quot; &quot;i&quot; &quot;t&quot; &quot;a&quot; &quot;i&quot; &quot;n&quot; &quot; &quot; &quot;(&quot; &quot;G&quot; &quot;B&quot; &quot;R&quot; &quot;)&quot; ## ## [[3]] ## [1] &quot;C&quot; &quot;h&quot; &quot;i&quot; &quot;n&quot; &quot;a&quot; &quot; &quot; &quot;(&quot; &quot;C&quot; &quot;H&quot; &quot;N&quot; &quot;)&quot; ## ## [[4]] ## [1] &quot;R&quot; &quot;u&quot; &quot;s&quot; &quot;s&quot; &quot;i&quot; &quot;a&quot; &quot; &quot; &quot;(&quot; &quot;R&quot; &quot;U&quot; &quot;S&quot; &quot;)&quot; ## ## [[5]] ## [1] &quot;G&quot; &quot;e&quot; &quot;r&quot; &quot;m&quot; &quot;a&quot; &quot;n&quot; &quot;y&quot; &quot; &quot; &quot;(&quot; &quot;G&quot; &quot;E&quot; &quot;R&quot; &quot;)&quot; ## ## [[6]] ## [1] &quot;J&quot; &quot;a&quot; &quot;p&quot; &quot;a&quot; &quot;n&quot; &quot; &quot; &quot;(&quot; &quot;J&quot; &quot;P&quot; &quot;N&quot; &quot;)&quot; It returns a list. So in future analyses, we will use the lapply() function to apply the same function to each element in the list. Let’s write our first function. It takes a vector (x), looks for a parenthesis, if there is one in x, then it limits x to only those items in the vector which came 2 before the parenthesis. keep_before_parenthesis &lt;- function(x){ # identify parenthesis location parenthesis_location &lt;- which(x == &quot;(&quot;) # if there is no parenthesis location (length == 0), then just output the original vector if(length(parenthesis_location) == 0){ return(x) } else { # otherwise # set the end point of x to be 2 before the location of the parenthesis end_point &lt;- parenthesis_location-2 # and index x x &lt;- x[1:end_point] } # output the new x return(x) } We apply this function to every item in the split_countries list with lapply split_countries &lt;- lapply(split_countries, keep_before_parenthesis) head(split_countries) ## [[1]] ## [1] &quot;U&quot; &quot;n&quot; &quot;i&quot; &quot;t&quot; &quot;e&quot; &quot;d&quot; &quot; &quot; &quot;S&quot; &quot;t&quot; &quot;a&quot; &quot;t&quot; &quot;e&quot; &quot;s&quot; ## ## [[2]] ## [1] &quot;G&quot; &quot;r&quot; &quot;e&quot; &quot;a&quot; &quot;t&quot; &quot; &quot; &quot;B&quot; &quot;r&quot; &quot;i&quot; &quot;t&quot; &quot;a&quot; &quot;i&quot; &quot;n&quot; ## ## [[3]] ## [1] &quot;C&quot; &quot;h&quot; &quot;i&quot; &quot;n&quot; &quot;a&quot; ## ## [[4]] ## [1] &quot;R&quot; &quot;u&quot; &quot;s&quot; &quot;s&quot; &quot;i&quot; &quot;a&quot; ## ## [[5]] ## [1] &quot;G&quot; &quot;e&quot; &quot;r&quot; &quot;m&quot; &quot;a&quot; &quot;n&quot; &quot;y&quot; ## ## [[6]] ## [1] &quot;J&quot; &quot;a&quot; &quot;p&quot; &quot;a&quot; &quot;n&quot; Now let’s paste them all back together. paste0 with the collapse argument specified with collapse all of the strings in a vector into one string separated by whatever character follows the collapse argument. split_countries &lt;- lapply(split_countries, function(x) paste0(x, collapse = &quot;&quot;)) head(split_countries) ## [[1]] ## [1] &quot;United States&quot; ## ## [[2]] ## [1] &quot;Great Britain&quot; ## ## [[3]] ## [1] &quot;China&quot; ## ## [[4]] ## [1] &quot;Russia&quot; ## ## [[5]] ## [1] &quot;Germany&quot; ## ## [[6]] ## [1] &quot;Japan&quot; We can use unlist to convert this list into a vector. split_countries &lt;- unlist(split_countries) head(split_countries) ## [1] &quot;United States&quot; &quot;Great Britain&quot; &quot;China&quot; &quot;Russia&quot; ## [5] &quot;Germany&quot; &quot;Japan&quot; And we assign the result to the medals_info dataset medals_info$Country &lt;- split_countries Now that both datasets have the same way of writing country names, we can merge the data by Country. merged_df &lt;- merge(gdp_info, medals_info, by = &quot;Country&quot;) head(merged_df) ## Country Rank.x GDP per personemployed(current Intl. $) ## 1 Algeria 64 29,997 ## 2 Argentina 56 34,772 ## 3 Armenia 82 18,812 ## 4 Australia 22 75,855 ## 5 Austria 16 80,982 ## 6 Azerbaijan 79 19,890 ## Employed /total pop.(%) Year Rank.y NOC Gold Silver Bronze Total ## 1 28.0 2010 63 Algeria (ALG) 0 2 0 2 ## 2 41.9 2009 27 Argentina (ARG) 3 1 0 4 ## 3 32.4 2008 42 Armenia (ARM) 1 3 0 4 ## 4 50.3 2010 10 Australia (AUS) 8 11 10 29 ## 5 49.4 2010 78 Austria (AUT) 0 0 1 1 ## 6 47.4 2009 39 Azerbaijan (AZE) 1 7 10 18 And we can plot results using ggplot, just like we did in our first R lesson! First, let’s select the columns we want and rename column 2 so it is easy to access with ggplot. merged_df &lt;- merged_df[,c(1,3,8)] colnames(merged_df)[2] &lt;- &quot;GDP_per_person_employed&quot; Now let’s plot. ggplot(merged_df, aes(x = GDP_per_person_employed, y = Gold, label = Country)) + geom_text() + geom_smooth(method=&#39;lm&#39;, formula= y~x) This plot is terrible! What is up with the x-axis? It doesn’t seem to be detecting either axis as numbers. Let’s convert them using as.numeric so that R doesn’t get confused again. There is a problem though.. the GDP values have commas in them. R won’t recognize them as potential numbers and will return NA when we run as.numeric. We can use the gsub function to substitute things in a string for something else. Let’s use it to remove the commas by substituting commas with empty strings. merged_df$GDP_per_person_employed &lt;- gsub(&quot;,&quot;, &quot;&quot;, merged_df$GDP_per_person_employed) Now we should be able to use as.numeric and plot the results. merged_df$GDP_per_person_employed &lt;- as.numeric(merged_df$GDP_per_person_employed) ggplot(merged_df, aes(x = GDP_per_person_employed, y = Gold, label = Country)) + geom_text() + geom_smooth(method=&#39;lm&#39;, formula= y~x) + xlab(&quot;GDP per person employed&quot;) Clearly GDP per person is not the only factor that affects which country will win the most golds at the Olympics. What important variables are we missing? "],["machine-learning-i-linear-and-multiple-regression.html", "8 Machine Learning I: Linear and Multiple Regression 8.1 More with the GSS 8.2 How regression works 8.3 Linear regression using the lm function 8.4 Prediction and prediction errors 8.5 Multiple Regression 8.6 Prediction", " 8 Machine Learning I: Linear and Multiple Regression In machine learning, we use statistical and algorithmic methods to detect patterns in our data. There are, generally, three goals: the first is to describe or understand our data. What are the patterns? How can we make sense of them? The second is to model our data - that is to test a theory on our data about how it was generated and to evaluate the accuracy of that theory. Related, to two, the third goal is to make predictions - if our model is generalizable, then it should apply to other, similar situations. There are a number of strategies for identifying and modeling patterns in data. In this regard, machine learning differs from more traditional ways of instructing computers on how to do things in that it provides a framework for the machine to learn from data on its own. We don’t have to type in what the machine should do at every step - it will be able to identify patterns using a model of choice across many different domains. We will focus this week on Linear Regression, the most common strategy in the social sciences for modeling data. 8.1 More with the GSS In the previous lab, we learned how to download data and visualize patterns between variables. In what follows, we will go beyond data visualization and begin to ask theoretically informed questions and using data, again, from the GSS, to answer those questions. Where before we plotted two variables against each other to see their relationship, linear regression will allow us to quantify their relationship: how much do changes in our explanatory variable lead to changes in the variable we are hoping to explain? Is this relationship statistically significant - that is, does it differ from what we should expect by chance? Linear regression will also allow us to adjust for covariates - variables that may be affecting our primary dependent variable of interest as well as our independent variable. For example, in classic example of confounding, we may see that the number of ice cream cones people eat per year is correlated with the number of sunburns they get and think that ice cream causes sunburns. However, it is obvious that both of these factors will be influenced by how warm of a climate people live in - with people living in warmer climates consuming more ice cream AND getting more sunburns. By controlling for the warmth of the climate, we can adjust for this fact and likely any association we saw between ice cream and sunburns will go away. 8.2 How regression works You may remember the slope-intercept form of writing the equation of a straight line from algebra. \\[ y = mx + b \\] Here, we can calculate the coordinate \\(y\\) for any \\(x\\) by first multiplying \\(x\\) by the slope of the line, \\(m\\), and adding the value of the intercept, \\(b\\), which is where the line intersects with the \\(y\\) axis. Linear regression is a strategy for finding the line that best fits the relationship between two variables. We start with a y variable, also called the outcome or the dependent variable, and an x variable, also called a predictor or the independent variable, and ask what is the slope-intercept equation that most closely approximates their relationship. Given \\(x\\) and \\(y\\), linear regression therefore involves estimating the slope, \\(m\\), and intercept, \\(b\\), of the line. Rarely in real world applications are two variables perfectly related to one another: even the best social science models have error. To reflect this, we update the equation above to: \\[ y = mx + b + ε \\] With \\(ε\\) capturing the error in our predictions. How do we fit a line to x and y? The short of it is that we will start with line (say, a horizontal one) and keep adjusting the slope and intercept of that line to minimize the average distance between the data points and the line itself. To see how this works, we can use the plot_ss function, taken from the statsr package. We will explore the relationship between age and income. Make sure you have the GSS data in your R environment. First, run this to load the function into your R environment plot_ss &lt;- function (x, y, data, showSquares = FALSE, leastSquares = FALSE) { missingargs &lt;- missing(x) | missing(y) | missing(data) if (missingargs) stop(simpleError(&quot;missing arguments x, y or data&quot;)) xlab &lt;- paste(substitute(x)) ylab &lt;- paste(substitute(y)) x &lt;- eval(substitute(x), data) y &lt;- eval(substitute(y), data) data = na.omit(data.frame(x = x, y = y)) x = data[[&quot;x&quot;]] y = data[[&quot;y&quot;]] plot(y ~ x, data = data, pch = 16, xlab = xlab, ylab = ylab) if (leastSquares) { m1 &lt;- lm(y ~ x, data = data) y.hat &lt;- m1$fit } else { cat(&quot;Click two points to make a line.&quot;) pt1 &lt;- locator(1) points(pt1$x, pt1$y, pch = 4) pt2 &lt;- locator(1) points(pt2$x, pt2$y, pch = 4) pts &lt;- data.frame(x = c(pt1$x, pt2$x), y = c(pt1$y, pt2$y)) m1 &lt;- lm(y ~ x, data = pts) y.hat &lt;- predict(m1, newdata = data) } r &lt;- y - y.hat abline(m1) oSide &lt;- x - r LLim &lt;- par()$usr[1] RLim &lt;- par()$usr[2] oSide[oSide &lt; LLim | oSide &gt; RLim] &lt;- c(x + r)[oSide &lt; LLim | oSide &gt; RLim] n &lt;- length(y.hat) for (i in 1:n) { lines(rep(x[i], 2), c(y[i], y.hat[i]), lty = 2, col = &quot;#56B4E9&quot;) if (showSquares) { lines(rep(oSide[i], 2), c(y[i], y.hat[i]), lty = 3, col = &quot;#E69F00&quot;) lines(c(oSide[i], x[i]), rep(y.hat[i], 2), lty = 3, col = &quot;#E69F00&quot;) lines(c(oSide[i], x[i]), rep(y[i], 2), lty = 3, col = &quot;#E69F00&quot;) } } SS &lt;- round(sum(r^2), 3) cat(&quot;\\r &quot;) print(m1) cat(&quot;Sum of Squares: &quot;, SS) } plot_ss(x = age, y = realinc, data = gss) ## Click two points to make a line. ## Call: ## lm(formula = y ~ x, data = pts) ## ## Coefficients: ## (Intercept) x ## 30130.57 74.27 ## ## Sum of Squares: 2.082262e+12 After running this command, you’ll be prompted to click two points on the plot to define a line. Once you’ve done that, the line you specified will be shown in black and the residuals in blue. Note that there is one residual for each of the observations. Residuals are the difference between the observed values and the values predicted by the line: \\[residual_i =y_i−ŷ_i\\] Linear regression will seek to minimize the sum of the squared residuals, also known as the sum of squares: \\[SumSquares = \\sum_{i = 1}^{n} (y_i−ŷ_i)^2\\] Using plot_ss, try minimizing the sum of squares. To do so, run the function multiple times, keeping track of the sum of squares that it returns, and adjusting your line to make it smaller. What was the smallest value you could achieve? 8.3 Linear regression using the lm function Thankfully, as computational social scientists, we won’t have to do this adjustment by hand. The lm function in R uses an algorithm, called gradient descent, to find the linear regression line that best minimizes the sum of squares for us. m1 &lt;- lm(realinc ~ age, data = gss) The first argument in the function lm is a formula that takes the form y ~ x. Here it can be read that we want to make a linear model of real household income as a function of age The second argument specifies that R should look in the gss data frame to find the age and realinc variables. The output of lm is an object that contains all of the information we need about the linear model that was just fit. We can access this information using the summary function. summary(m1) ## ## Call: ## lm(formula = realinc ~ age, data = gss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36142 -21953 -8800 11184 88412 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30130.57 1968.22 15.309 &lt;2e-16 *** ## age 74.27 37.87 1.961 0.05 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31160 on 2145 degrees of freedom ## (201 observations deleted due to missingness) ## Multiple R-squared: 0.00179, Adjusted R-squared: 0.001324 ## F-statistic: 3.846 on 1 and 2145 DF, p-value: 0.04999 Let’s consider this output piece by piece. First, the formula used to describe the model is shown at the top. After the formula you find the five-number summary of the residuals. The “Coefficients” table shown next is key; its first column displays the linear model’s y-intercept and the coefficient of age. With this table, we can write down the least squares regression line for the linear model: \\[ŷ=30130.57+74.27∗age\\] It also shows whether the coefficients, here, age, have are statistically significant in predicting the outcome, income. Normally, a p-value cut-off of 0.05 is used to determine statistical significance - here, age’s p-value is almost exactly 0.05 (in fact, it is very slightly lower) and is therefore significant. One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, R2. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, only %.18 of the variability in income is explained by age. What variables might do a better job of explaining income? Let’s try years of education, which is the variable educ in the gss.Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between income and education? m2 &lt;- lm(realinc ~ educ, data = gss) summary(m2) ## ## Call: ## lm(formula = realinc ~ educ, data = gss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -57666 -18112 -6093 10864 97362 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21552 3006 -7.171 1.02e-12 *** ## educ 4006 213 18.809 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28880 on 2149 degrees of freedom ## (197 observations deleted due to missingness) ## Multiple R-squared: 0.1414, Adjusted R-squared: 0.141 ## F-statistic: 353.8 on 1 and 2149 DF, p-value: &lt; 2.2e-16 \\[y=-21552+4006∗educ\\] The higher one’s education, the higher one’s income, generally. Specifically, the slope tells us that for every year of education, a person is expected to see an additional income of 4006 dollars. Further, the intercept tells us that people with 0 years of education are expect to have an income of -21552 dollars. Of course, this is an extrapolation, produced by the linear regression: an income of negative dollars doesn’t make much sense. Finally, we can see from the regression output that the R2 for education is much higher than age: 11%! 8.4 Prediction and prediction errors Let’s create a scatterplot with the least squares line laid on top. library(ggplot2) ggplot(gss, aes(x = educ, y = realinc)) + geom_point()+ geom_smooth(method=&#39;lm&#39;, formula= y~x) ## Warning: Removed 197 rows containing non-finite values (stat_smooth). ## Warning: Removed 197 rows containing missing values (geom_point). 8.5 Multiple Regression We can build more complex models by adding variables to our model. When we do that, we go from the world of simple regression to the more complex world of multiple regression. Here, for example, is a more complex model with both of the variables we examined above. m3 &lt;- lm(realinc ~ age + educ, data = gss) summary(m3) ## ## Call: ## lm(formula = realinc ~ age + educ, data = gss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60809 -19200 -6908 10063 99951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -25708.70 3481.35 -7.385 2.18e-13 *** ## age 83.51 35.11 2.378 0.0175 * ## educ 4012.21 212.96 18.840 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28870 on 2143 degrees of freedom ## (202 observations deleted due to missingness) ## Multiple R-squared: 0.1436, Adjusted R-squared: 0.1428 ## F-statistic: 179.7 on 2 and 2143 DF, p-value: &lt; 2.2e-16 What do we learn? How does the Multiple R-Squared look compard to previous models? Did we improve our model fit? By including both variables, the coefficients now tell us the effect of each variable, conditional on the other. For example, age and education are correlated in complex ways. Younger generations are slightly more educated than older generations. At the same time, being older gives you more time to achieve an education. Controlling for age allows us to interpret education without worrying that age effects of this sort are driving the relationship between education and income. That said, after age is controlled for, while R2 improves, the coefficient size for education stays roughly the same. 8.6 Prediction Finally, imagine that you want to predict your own income when you graduate in the coming years using this model. Let’s collect data on what your age and education will be at the time of graduation and use the line to predict your future income. First, we create a data.frame with the necessary data. you &lt;- data.frame(Name = &quot;YOURNAMEHERE&quot;, age = 22, educ = 16) View(you) As long as a data.frame has the same variables as those in a model, we can use the predict function to predict the expected outcomes of the respondents in the data.frame using the model. What is your predicted income? predict(m3, you) ## 1 ## 40323.84 To get a better prediction, more tailored to you and your background, we would need a more complex model with more variables! "],["machine-learning-with-caret.html", "9 Machine Learning with caret 9.1 The data 9.2 Training vs. testing 9.3 Creating features 9.4 Constructing a model", " 9 Machine Learning with caret A few weeks back, we learned how to perform linear regression, with single and multiple predictors. Linear regression will be useful in a wide range of situations, particularly for descriptive and scientific purposes when you want outputs that you can interpret. That said, there are no guarantees that it will be the most accurate method for predicting your outcome of interest. In fact, often times, when you want to optimize on prediction accuracy, it is worth trying a range of other methods to compare which provides the most accurate predictions. The best package for doing this in R is caret (short for Classification And REgression Training). It provides: tools for splitting your data into training and test sets, a number of different models, from random forests to logistic regression, and tools for selecting the best model out of a set of alternatives. In this brief tutorial, we will go through an example from start to finish. We’ll load data, identify an outcome of interest, split the data into training and test sets, fit a series of models to the training data, and evaluate their performance on the test set. Machine learning is an entire subfield of computer science and could be the basis of a course on its own, as such we will only graze the surface of what you can do and what you should consider when predicting social phenomena. 9.1 The data The data for this week’s lab come from Twitter, specifically from a recent Kaggle competition (https://www.kaggle.com/c/twitter-sentiment-analysis2), which asked participants to predict the sentiment of a series of tweets using the text content of the tweets. Given our discussion of text analysis in class, let’s see how accurately we can predict sentiment using the tools at our disposal. 9.2 Training vs. testing In machine learning, people generally separate their analysis into two steps. First, they train their model on the data. This is what we did when we fit a multiple regression model to the GSS data. Second, they test the model on data the model hasn’t seen yet in order to evaluate its performance. Why does it matter if the model has seen the test data before? Often times, this can lead to overfitting - where your model is trained too specifically to the data at hand and cannot generalize beyond it. This can means that when your model does see new data it hasn’t seen before, it will do a poor job predicting. We will follow this general routine, and train our models on a subset of the data called the training data and then test how well it performs by seeing how accurately it classifies new data. Generally, you assign about a fifth of your total sample to be part of the testing data and the remaining eighty percent are used to train the model. First, let’s install caret and load it into R. We’ll need to load in tidytext and tidyverse too. install.packages(&quot;caret&quot;) library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift library(tidyverse) Great, now let’s load in the data. I put it on Canvas for easy access. data &lt;- read.csv(&quot;Data/train.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) We’ll focus specifically on the SentimentText column, which includes the content of tweets. This is our first time working with text data and it is never as easy as you would hope. We have to do a few things to prepare the data; for example, convert it to utf-8 and remove in extra white space. For the first, we’ll use iconv, which converts between different standards. For the second, we’ll use trimws, which, you guessed it, trims white space. data$SentimentText &lt;- iconv(data$SentimentText, from = &quot;latin1&quot;, to = &quot;utf-8&quot;) data$SentimentText &lt;- trimws(data$SentimentText) Okay now let’s have a look at the data. View(data) It is pretty simple - there are three columns, an id column, the human-evaluated sentiment of the text, and the actual text itself. The data are huge, so we can play around with a smaller subset of 10000 tweets. set.seed(1234) data &lt;- data[sample(1:nrow(data), 1000),] 9.3 Creating features To predict the sentiment, we will have to use the text of the tweets. Specifically, we need to extract characterizations or features for each of the tweets which can then be used as predictor variables in the model. We could for example use the different words in the tweets as predictors (how many times does the word happy appear in a given tweet? how about sad?). The problem with this approach is that most words only appear in a few tweets at most. Having such sparse predictors can lead to unstable models which fail to converge. That means we would have to be very selective in our choices and there is no guarantee that we would choose the right words. So what about adding some overall features of the tweet, rather than just the words themselves? There is a cool package called textfeatures in R, which automatically extracts a bunch of useful text information for you (even word embeddings!) install.packages(&quot;textfeatures&quot;) install.packages(&quot;text2vec&quot;) library(textfeatures) library(text2vec) Let’s extract the features along with 20 word embedding dimensions textlevel_features &lt;- textfeatures(data$SentimentText, word_dims = 20, normalize = T, verbose = FALSE) data_w_features &lt;- cbind(data, textlevel_features) # drop the coluns which we won&#39;t use in the model data_w_features &lt;- subset(data_w_features, select = -c(ItemID, SentimentText)) # set the outcome variable to factor (since it is categorical, sentiment or not) data_w_features$Sentiment = as.factor(data_w_features$Sentiment) ## drop columns with little to no variance min_var &lt;- function(x, min = 1) { is_num &lt;- vapply(x, is.numeric, logical(1)) non_num &lt;- names(x)[!is_num] yminvar &lt;- names(x[is_num])[vapply(x[is_num], function(.x) stats::var(.x, na.rm = TRUE) &gt;= min, logical(1))] x[c(non_num, yminvar)] } data_w_features &lt;- min_var(data_w_features) Now let’s split this smaller data into training and test sets. training_ids &lt;- sample(rownames(data_w_features), 800) training_data &lt;- data_w_features[rownames(data_w_features) %in% training_ids,] test_data &lt;- data_w_features[!rownames(data_w_features) %in% training_ids,] Cool! Now let’s begin building our models. 9.4 Constructing a model Constructing models in caret involves two steps. First, we have to decide how training should occur. For example, should it use cross-validation (where the data is divided into k equal parts, k rounds of training and testing occur, where data is trained on k-1 portions of the data and tested on the last, held-out portion)? There are many different options, but for now we will just use cross-validation with 5 folds. fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 5) Now we can fit models. m.randomForest &lt;- train(Sentiment ~ ., data = training_data, method = &quot;rf&quot;, trControl = fitControl, na.action = na.omit, trace = FALSE) ## Naive Bayes ## ## 800 samples ## 37 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 641, 640, 640, 640, 639 ## Resampling results across tuning parameters: ## ## usekernel Accuracy Kappa ## FALSE 0.5853086 0.2041723 ## TRUE 0.5988604 0.2304654 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = TRUE and adjust ## = 1. m.decisionTree &lt;- train(Sentiment ~ ., data = training_data, method = &quot;C5.0&quot;, trControl = fitControl, na.action = na.omit, trace = FALSE) m.decisionTree ## C5.0 ## ## 800 samples ## 37 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 640, 640, 640, 639, 641 ## Resampling results across tuning parameters: ## ## model winnow trials Accuracy Kappa ## rules FALSE 1 0.6174395 0.2195846 ## rules FALSE 10 0.6311663 0.2466009 ## rules FALSE 20 0.6387677 0.2580611 ## rules TRUE 1 0.6050568 0.1856155 ## rules TRUE 10 0.6036349 0.1844294 ## rules TRUE 20 0.6173536 0.2143403 ## tree FALSE 1 0.6049550 0.1971405 ## tree FALSE 10 0.6436352 0.2681289 ## tree FALSE 20 0.6674091 0.3171329 ## tree TRUE 1 0.5850023 0.1572419 ## tree TRUE 10 0.6037680 0.1875013 ## tree TRUE 20 0.6149632 0.2136201 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were trials = 20, model = tree and winnow ## = FALSE. m.NeuralNet &lt;- train(Sentiment ~ ., data = training_data, method = &quot;nnet&quot;, trControl = fitControl, na.action = na.omit, trace = FALSE) m.NeuralNet ## Neural Network ## ## 800 samples ## 37 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 640, 640, 640, 641, 639 ## Resampling results across tuning parameters: ## ## size decay Accuracy Kappa ## 1 0e+00 0.6251111 0.2324807 ## 1 1e-04 0.6413309 0.2799198 ## 1 1e-01 0.6536978 0.3034218 ## 3 0e+00 0.6225957 0.2290596 ## 3 1e-04 0.6325730 0.2541539 ## 3 1e-01 0.6062516 0.2038744 ## 5 0e+00 0.6025251 0.1941279 ## 5 1e-04 0.5938376 0.1701331 ## 5 1e-01 0.6149866 0.2246250 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were size = 1 and decay = 0.1. Which method makes the most accurate predictions? caret provides some handy functions for evaluating this. # Make predictions using the test data set rf.pred &lt;- predict(m.randomForest,test_data) nb.pred &lt;- predict(m.NaiveBayes,test_data) dt.pred &lt;- predict(m.decisionTree,test_data) nn.pred &lt;- predict(m.NeuralNet,test_data) #Look at the confusion matrix confusionMatrix(rf.pred, test_data$Sentiment) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 44 28 ## 1 36 92 ## ## Accuracy : 0.68 ## 95% CI : (0.6105, 0.744) ## No Information Rate : 0.6 ## P-Value [Acc &gt; NIR] : 0.01187 ## ## Kappa : 0.322 ## ## Mcnemar&#39;s Test P-Value : 0.38157 ## ## Sensitivity : 0.5500 ## Specificity : 0.7667 ## Pos Pred Value : 0.6111 ## Neg Pred Value : 0.7188 ## Prevalence : 0.4000 ## Detection Rate : 0.2200 ## Detection Prevalence : 0.3600 ## Balanced Accuracy : 0.6583 ## ## &#39;Positive&#39; Class : 0 ## confusionMatrix(nb.pred, test_data$Sentiment) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 69 68 ## 1 11 52 ## ## Accuracy : 0.605 ## 95% CI : (0.5336, 0.6732) ## No Information Rate : 0.6 ## P-Value [Acc &gt; NIR] : 0.4732 ## ## Kappa : 0.2644 ## ## Mcnemar&#39;s Test P-Value : 2.967e-10 ## ## Sensitivity : 0.8625 ## Specificity : 0.4333 ## Pos Pred Value : 0.5036 ## Neg Pred Value : 0.8254 ## Prevalence : 0.4000 ## Detection Rate : 0.3450 ## Detection Prevalence : 0.6850 ## Balanced Accuracy : 0.6479 ## ## &#39;Positive&#39; Class : 0 ## confusionMatrix(dt.pred, test_data$Sentiment) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 43 33 ## 1 37 87 ## ## Accuracy : 0.65 ## 95% CI : (0.5795, 0.7159) ## No Information Rate : 0.6 ## P-Value [Acc &gt; NIR] : 0.0844 ## ## Kappa : 0.2647 ## ## Mcnemar&#39;s Test P-Value : 0.7199 ## ## Sensitivity : 0.5375 ## Specificity : 0.7250 ## Pos Pred Value : 0.5658 ## Neg Pred Value : 0.7016 ## Prevalence : 0.4000 ## Detection Rate : 0.2150 ## Detection Prevalence : 0.3800 ## Balanced Accuracy : 0.6312 ## ## &#39;Positive&#39; Class : 0 ## confusionMatrix(nn.pred, test_data$Sentiment) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 52 47 ## 1 28 73 ## ## Accuracy : 0.625 ## 95% CI : (0.5539, 0.6923) ## No Information Rate : 0.6 ## P-Value [Acc &gt; NIR] : 0.25896 ## ## Kappa : 0.2485 ## ## Mcnemar&#39;s Test P-Value : 0.03767 ## ## Sensitivity : 0.6500 ## Specificity : 0.6083 ## Pos Pred Value : 0.5253 ## Neg Pred Value : 0.7228 ## Prevalence : 0.4000 ## Detection Rate : 0.2600 ## Detection Prevalence : 0.4950 ## Balanced Accuracy : 0.6292 ## ## &#39;Positive&#39; Class : 0 ## We can also plot a ROC curve, in which the True Positive rate (sensitivity) is plotted against the True Negative rate (specificity). This is good for evaluating whether your model is both correctly predicting which are and are not positive sentiment (not just one or the other). library(pROC) #Draw the ROC curve nn.probs &lt;- predict(m.NeuralNet,test_data,type=&quot;prob&quot;) nn.ROC &lt;- roc(predictor=nn.probs$`1`, response=as.numeric(test_data$Sentiment)-1, levels=rev(levels(test_data$Sentiment))) nn.ROC$auc ## Area under the curve: 0.6939 #Area under the curve: 0.6936 plot(nn.ROC,main=&quot;Neural Net ROC&quot;) &lt;!--chapter:end:09-MachineLearning.Rmd--&gt; --- output: html_document --- # A text project, from start to topic model In this tutorial, we focus on a new analysis strategy for text - topic modeling. We download all of the pages on Wikipedia for famous philosophers, from Aristotle to Bruno Latour. Each page discusses their lives and works and we topic model the text in order to identify shared themes across them. To do this, we first need to load in a bunch of packages. If you are missing one of these packages - if you get the error message &quot;Error in library(tm) : there is no package called ‘tm’&quot;, for example - then you should use install.packages() to install it. In particular, there are three packages we have never seen before: stringi, which is useful for manipulating strings (we will use it to convert non-latin script to latin script), tm, which is a suite of functions for text mining in R, and texclean, which has useful functions for cleaning text. ```r library(rvest) library(tidytext) library(dplyr) library(tidyr) library(stringi) library(tm) library(textclean) Most of the real scraping work I left out of the tutorial for the sake of time. But I followed, more or less, what we learned in the week on scraping. I found a page a series of pages which list philosophers alphabetically. I visited those pages and saw that they contained links to every Wikipedia page for a well-known philosopher. I used Inspect to copy the Xpath for a couple of these links and found that they followed a similar pattern - each was nested in the HTML structure underneath the pathway //*[@id=“mw-content-text”]/div/ul/li/a. I extracted the set of nodes which followed that path and grabbed the href (HTML lingo for a url) from each. The result was a list of Wikipedia short links for philosophers. I pasted the main wikipedia URL to precede short links. I also grabbed the titles of the nodes, which was the names of the philosophers. I used lapply to apply this function to each of the four pages, saved the results for each in a data.frame, and used do.call(“rbind”) to put all of them into a single data.frame. grab_urls &lt;- function(x){ # id the url locations on the page (identified using Inspect and XPath) philosophers_urls &lt;- html_nodes(x, xpath = &#39;//*[@id=&quot;mw-content-text&quot;]/div/ul/li/a&#39;) # extract the URL specifically href_links &lt;- philosophers_urls %&gt;% html_attr(&#39;href&#39;) # paste the url ending to wikipedia English main url urls &lt;- paste0(&quot;https://en.wikipedia.org/&quot;, href_links) # extract each philosopher&#39;s name philosopher_names &lt;- philosophers_urls %&gt;% html_attr(&#39;title&#39;) # save the result in a data.frame df &lt;- data.frame(Philosopher = philosopher_names, URL = urls, stringsAsFactors = F) # output the data.frame return(df) } # download html for each of the main philosopher pages from wikipedia philosophers_A_C &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_philosophers_(A–C)&quot;) philosophers_D_H &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_philosophers_(D–H)&quot;) philosophers_I_Q &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_philosophers_(I–Q)&quot;) philosophers_R_Z &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_philosophers_(R–Z)&quot;) # apply the above code to each of the pages all_dfs &lt;- lapply(list(philosophers_A_C, philosophers_D_H, philosophers_I_Q, philosophers_R_Z), grab_urls) # put it all together all_philosophers &lt;- do.call(&quot;rbind&quot;, all_dfs) # saveRDS(all_philosophers, &quot;all_philosophers_2021.RDS&quot;) Let’s take a look. The data.frame has two columns - Philosopher and URL. We can use this information to now go to each of the philosopher’s pages and grab the content of their Wikipedia page. View(all_philosophers) So now I write a new function which grabs the text from each page. It takes as its argument a URL. The HTML of this URL is read into R using rvest’s read_html. Then the body of the page - identified by //*[@id=“mw-content-text”]/div/p - is read into R and its text is extracted. This text is returned. grab_text &lt;- function(x){ philosopher_html &lt;- read_html(x) philosopher_text &lt;- philosopher_html %&gt;% html_nodes(xpath = &#39;//*[@id=&quot;mw-content-text&quot;]/div/p&#39;) %&gt;% html_text() return(philosopher_text) } I apply it, again using lapply, to every URL in the all_philosophers data.frame. The result is the text of every philosopher’s page on Wikipedia. The only problem is it takes a while to run, especially if your computer isn’t fast. philosophers_page_text &lt;- lapply(all_philosophers$URL, function(x) try(grab_text(x))) # saveRDS(philosophers_page_text, &quot;philosophers_page_text_2021.RDS&quot;) I actually saved the results into an RDS file and put them on Canvas, so that you wouldn’t have to run this full loop (though you can if you are curious.) Download philosophers_page_text.RDS, drag it into your R directory, and load it in using readRDS, like so. philosophers_page_text &lt;- readRDS(&quot;Data/philosophers_page_text_2021.RDS&quot;) philosophers_page_text &lt;- philosophers_page_text[sample(1:length(philosophers_page_text), 200)] So the texts are quite messy and we need to clean them before we can analyze them (though with topic modeling, this isn’t strictly necessary since it will often lump all of the junk into its own topic.) We build a function to do that. It uses repeated gsubs to remove characters that we don’t want from the text. If you don’t really understand what is going on here, then it is worth reading up on regex - it is an essential framework for working with text. Once the text is cleaned, we put all of the sentences for each philosopher into a single character vector and make it lowercase. Finally, we convert the list of texts to a character vector clean_text &lt;- function(x){ # remove all parentheses and their content x &lt;- gsub(&quot;\\\\s*\\\\([^\\\\)]+\\\\)&quot;, &quot;&quot;, x) # remove all square brackets and their content x &lt;- gsub(&quot;\\\\[[^][]*]&quot;, &quot;&quot;, x) # remove all punctuation x &lt;- gsub(&#39;[[:punct:] ]+&#39;,&#39; &#39;,x) # remove all numbers x &lt;- gsub(&#39;[[:digit:]]+&#39;, &#39;&#39;, x) # drop paragraph breaks x &lt;- gsub(&#39;\\n&#39;, &#39;&#39;, x) # drop empty lines x &lt;- subset(x, x != &quot;&quot;) # remove spaces at beginning and end of lines x &lt;- trimws(x) # paste all of the lines together into one text x &lt;- paste0(x, collapse = &quot; &quot;) # make everything lower case x &lt;- tolower(x) # return text return(x) } philosophers_texts_cleaned &lt;- unlist(lapply(philosophers_page_text, clean_text)) Now we can add the texts into the data.frame of philosophers and their URLs. We can also drop the philosophers whose name was on Wikipedia but who don’t actually have a page. They can be identified by the fact that their text equals “error in open connection http error”. # convert non-latin script to latin, if no easy conversion, then substite for empty string (i.e. delete) philosophers_texts_cleaned &lt;- iconv(stri_trans_general(philosophers_texts_cleaned, &quot;latin-ascii&quot;), &quot;latin1&quot;, &quot;ASCII&quot;, sub=&quot;&quot;) philosophers_texts_cleaned &lt;- replace_non_ascii(philosophers_texts_cleaned) good_texts &lt;- which(philosophers_texts_cleaned!= &quot;error in open connection http error&quot;) all_philosophers &lt;- all_philosophers[good_texts,] all_philosophers$Text &lt;- philosophers_texts_cleaned[good_texts] Now we have to do some more cleaning. We can turn this data set into a tokenized tidytext data set with the unnest_tokens function. # further cleaning... # tokenize the result text_cleaning_tokens &lt;- all_philosophers %&gt;% unnest_tokens(word, Text) Next we want to drop words which are less than three characters in length, and drop stop words. We can drop short words with filter combined with the nchar function, and anti_join to drop stopwords. # drop words with are either stop words or length == 1 text_cleaning_tokens &lt;- text_cleaning_tokens %&gt;% filter(!(nchar(word) &lt; 3)) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; Next we drop empty words # filter out empty words tokens &lt;- text_cleaning_tokens %&gt;% filter(!(word==&quot;&quot;)) The next part is a bit complicated. The basic idea is that we want to paste the texts for each philosopher back together. The unite function is good for that, but it only works on a wide form data set. So we will first group by philosopher, produce an index for the row number (that is, what position is a given word in their text), we will then spread the data, converting our long form data into wide form, setting the key argument (which defines the columns of the new data.frame) to equal the index we created, and the value argument to word. The result is that each book is now its own row in the data.frame, with the column i+2 identifying the ith word in that philosopher’s Wikipedia page. # turn each person into a row, with a column for each word in their wikipedia tokens &lt;- tokens %&gt;% group_by(Philosopher) %&gt;% mutate(ind = row_number()) %&gt;% spread(key = ind, value = word) We’ll convert NAs to \"\" and use unite to paste all of the columns in the data.frame together. We specify -Philosopher and -URL so that those columns are preserved and not pasted with the words of each page. # convert NAs to empty strings tokens[is.na(tokens)] &lt;- &quot;&quot; # put the data.frame back together tokens &lt;- unite(tokens, Text,-Philosopher, -URL, sep =&quot; &quot; ) Two last things are necessary before we analyze the data. We need to trim whitespace, so that there aren’t spaces at the beginning or end of texts. And we need to convert non-latin characters to latin characters (using the stri_trans_general() function from stringi) or else, if they can’t be converted, drop them (using the iconv() function from base R.) # trim white space tokens$Text &lt;- trimws(tokens$Text) Great! Let’s check out the data. View(tokens) Topic modeling requires a document to word matrix. In such a matrix, each row is a document, each column is a word, and each cell or value in the matrix is a count of how many times a given document uses a given word. To get to such a matrix, we first need to count how many times each word appears on each philosopher’s page. We learned how to do this last tutorial. token_counts &lt;- tokens %&gt;% unnest_tokens(word, Text) %&gt;% count(Philosopher, word, sort = TRUE) Now we can use a function called cast_dtm from the tm package to convert token_counts into a document-to-word matrix (dtm stands for document-to-term, actually.) We tell it - the variable in token_counts we want to use for the rows of the dtm (Philosopher), the variable we want to use as the column (word), and the variable that should fill the matrix as values (n). philosopher_dtm &lt;- token_counts %&gt;% cast_dtm(Philosopher, word, n) Awesome! You can View what it looks like if you want. We can use this dtm to fit a topic model using latent dirichlet allocation (LDA) from the topicmodels package. We have a few options when doing so - first we will set k, the number of topics to equal 20. If we were doing this for a real study (like your final project), we would want to fit a couple of different models with different ks to see how the results change and to try to find the model with the best fit. For now, we will settle for just trying k = 20. We can also set the seed directly inside the function so that we are certain to all get the same results. This might take a while! library(topicmodels) ## ## Attaching package: &#39;topicmodels&#39; ## The following object is masked from &#39;package:text2vec&#39;: ## ## perplexity philosophers_lda &lt;- LDA(philosopher_dtm, k = 20, control = list(seed = 1234)) It finished running - now what? We can use the tidy function from the tidyr package to extract some useful information. First, let’s extract the beta coefficients - which provides weights for the words with respect to topics. philosophers_lda_td &lt;- tidy(philosophers_lda, matrix = &quot;beta&quot;) Just like we did last class, let’s use top_n to grab the top 10 words per topic. top_terms &lt;- philosophers_lda_td %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) We can plot the results using ggplot as a series of bar plots library(ggplot2) top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta)) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + facet_wrap(~ topic, scales = &quot;free_x&quot;) + theme_bw() Or else as a table. top_terms_table &lt;- top_terms %&gt;% group_by(topic) %&gt;% mutate(order = 1:length(topic)) %&gt;% select(-beta) %&gt;% spread(topic, term) %&gt;% select(-order) Table 9.1: Top 10 terms per topic 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 searle confucius darwin chomsky output bakhtin adorno hazlitt junger sun avicenna epicurus gaddafi thoreau descartes stalin wundt linnaeus watts luxemburg deleuze mahavira philosophy rand parser losurdo bonhoeffer coleridge anaximenes tocqueville philosophy mather proudhon gambra world soviet psychology tolstoy philosophy arnold beauvoir philosophy university political royce herder university life air china anaximander skinner libya monboddo philosophy lenin eliot hus eco published philosophy plantinga species baudrillard philosophy language german time scholem sen world behavior property hesiod ideas war university church university russian women holbach book philosophy madhva history philosophy longinus polanyi yat islamic philosophy arab john malebranche union philosophy melanchthon book german astell university natural jevons god oken music essays war martineau koyre book government time mind party theory clarke gentile philosophy university family howison theory madhvacharya world church wordsworth output political science santayana boole wrote god russian chateaubriand luther life stein published jain selection university lock published published literary parser chinese aristotle life political walden hutton government koffka time published germany french teachings theory published citation time time published world freire mill pyrrho libyan modern herbart germany published university time alexander derrida century published language agrippa german culture william theory education medicine epicureanism revolution life body communist psychological published boyle university NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA georgian NA NA NA NA What if want to look at the extent to which each document or philosopher is composed of each topic? We can instead set matrix = “gamma” to get the gamma values, which tell you exactly that. philosophers_lda_gamma &lt;- tidy(philosophers_lda, matrix = &quot;gamma&quot;) We’ll sort in descending order according to gamma top_topics &lt;- philosophers_lda_gamma %&gt;% arrange(document, -gamma) There are a bunch of philosophers, too many to examine all at once. Let’s select a few particularly prominent ones and examine their topic distributions. selected_philosophers &lt;- c(&quot;Martin Luther King, Jr.&quot;, &quot;Francis Bacon&quot;, &quot;Wang Fuzhi&quot;, &quot;Aristotle&quot;, &quot;Immanuel Kant&quot;, &quot;Pope John XXI&quot;, &quot;Friedrich Kayek&quot;, &quot;Edmund Burke&quot;, &quot;Simone de Beauvoir&quot;, &quot;Bruno Latour&quot;) top_topics %&gt;% filter(document %in% selected_philosophers) %&gt;% ggplot(aes(topic, gamma)) + geom_bar(stat = &quot;identity&quot;) + scale_x_continuous(breaks=1:20) + facet_wrap(~ document, scales = &quot;free&quot;) + theme_bw() 9.4.1 LAB For the lab this week, select or randomly sample 100 texts from the Gutenberg library and topic model the texts with a k of your choosing. "],["social-and-semantic-network-analysis.html", "10 Social and Semantic Network Analysis 10.1 Understanding network data structures 10.2 Producing a skip-gram matrix for semantic network analysis and embedding models 10.3 From data to networks 10.4 Exploring your network 10.5 The Basics of Visualization 10.6 Layouts 10.7 Group detection", " 10 Social and Semantic Network Analysis This week our tutorial will cover network analysis. Network analysis focuses on patterns of relations between actors. Both relations and actors can be defined in many ways, depending on the substantive area of inquiry. For example, network analysis has been used to study the structure of affective links between persons, flows of commodities between organizations, shared members between social movement organizations, and shared words between texts. What is common across these domains is an emphasis on the structure of relations, which serves to link the micro- and macro-levels. 10.1 Understanding network data structures Underlying every network visualization is data about relationships. These relationships can be observed or simulated (that is, hypothetical). When analyzing a set of relationships, we will generally use one of two different data structures: edge lists or adjacency matrices. 10.1.1 Edge lists One simple way to represent a graph is to list the edges, which we will refer to as an edge list. For each edge, we just list who that edge is incident on. Edge lists are therefore two column matrices that directly tell the computer which actors are tied for each edge. In a directed graph, the actors in column A are the sources of edges, and the actors in Column B receive the tie. In an undirected graph, order doesn’t matter. In R, we can create an example edge list using vectors and data.frames. I specify each column of the edge list with vectors and then assign them as the columns of a data.frame. We can use this to visualize what an edge list should look like. personA &lt;- c(&quot;Mark&quot;, &quot;Mark&quot;, &quot;Peter&quot;, &quot;Peter&quot;, &quot;Bob&quot;, &quot;Jill&quot;) personB &lt;- c(&quot;Peter&quot;, &quot;Jill&quot;, &quot;Bob&quot;, &quot;Aaron&quot;, &quot;Jill&quot;, &quot;Aaron&quot;) edgelist &lt;- data.frame(PersonA = personA, PersonB = personB, stringsAsFactors = F) print(edgelist) ## PersonA PersonB ## 1 Mark Peter ## 2 Mark Jill ## 3 Peter Bob ## 4 Peter Aaron ## 5 Bob Jill ## 6 Jill Aaron What are the upsides of using the edge list format? As you can see, in an edge list, the number of rows accords to the number of edges in the network since each row details the actors in a specific tie. It is therefore really simple format for recording network data in an excel file or csv file. What are the downsides? The first is practical - it is impossible to represent isolates using an edge list since it details relations. There are ways to get around this problem in R, however. The second is technical - edge lists are not suitable for data formats for performing linear algebraic techniques. As a result, we will almost always convert and edge list into either an adjacency matrix, or into a network object. 10.1.2 Adjacency matrices Adjacency matrices have one row and one column for each actor in the network. The elements of the matrix can be any number but in most networks, will be either 0 or 1. A matrix element of 1 (or greater) signals that the respective column actor and row actor should be tied in the network. Zero signals that they are not tied. We can use what we learned in the last tutorial to create such a matrix. An example might look like this: adjacency &lt;- matrix(c(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0), nrow = 5, ncol = 5, dimnames = list(c(&quot;Mark&quot;, &quot;Peter&quot;, &quot;Bob&quot;, &quot;Jill&quot;, &quot;Aaron&quot;), c(&quot;Mark&quot;, &quot;Peter&quot;, &quot;Bob&quot;, &quot;Jill&quot;, &quot;Aaron&quot;))) print(adjacency) ## Mark Peter Bob Jill Aaron ## Mark 0 1 0 1 0 ## Peter 1 0 1 0 1 ## Bob 0 1 0 1 0 ## Jill 1 0 1 0 1 ## Aaron 0 1 0 1 0 What are the upsides of using the adjacency matrix format? Adjacency matrices are the most fundamental network analysis data format. They undergird all of the analytical techniques we will show you later on. They are also much more efficient than edge lists. For example, imagine searching for whether Peter and Jill are friends in an adjacency matrix as opposed to an edge list. In the adjacency matrix, we would go to Peter’s row and Jill’s column and we would find either a 1 or a 0, giving us our answer. In an edge list, we would have to search through each edge, which might seem simple in a dataset with only 5 people, but as the edge list grows, it will scale linearly with the number of edges. What are the downsides? It is really difficult to record network data with an adjacency matrix. They are better suited for computers than people. 10.2 Producing a skip-gram matrix for semantic network analysis and embedding models In previous lessons, we learned how to fit topic models to many different texts (for example, the Wikipedia pages of philosophers). But what if, as is often the case, you only have a single text, and want to visualize its internal structure? Or, as is the case in embedding models, you want to model not just the set of words that each text has, but rather, the precise sequences of words that are employed in each text. Doing so should reveal not just composition, but how words relate to each other within and across texts; or put differently, the local context in which words appear. To produce such models, we need a different kind of data from the raw co-occurence matrices we made use of before. We will use what is called a skip-gram model, which counts the number of times a given word appears near another word, with near-ness being defined as some kind of window, say, of 4 or 5 words. A skip-gram window of two, for example, would count the number of times word j appears within the two words immediately before or after word i. Rather than building a skip-gram model ourselves, we can just use the CreateTcm() function from the textmineR package of last lesson to turn a text of interest into a skip-gram. First, let’s install the textmineR package, load in some other necessary packages, and grab a text, The Constitution of the United States of America, from the Gutenberg Project to play around with. install.packages(&quot;textmineR&quot;) # Load in packages library(gutenbergr) library(tidyverse) library(tidytext) library(textmineR) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;textmineR&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## update ## The following object is masked from &#39;package:topicmodels&#39;: ## ## posterior ## The following object is masked from &#39;package:stats&#39;: ## ## update # Download the constitution constitution &lt;- gutenberg_download(c(5), meta_fields = &quot;title&quot;) ## Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest ## Using mirror http://aleph.gutenberg.org # Tokenize the constitution tokenized_words &lt;- unnest_tokens(constitution, word, text, strip_punct = F) # Load in stop words data(stop_words) # Drop stop words as well as useless puntuation word_counts &lt;- anti_join(tokenized_words, stop_words, by = &quot;word&quot;) word_counts &lt;- anti_join(word_counts, data.frame(word = c(&quot;*&quot;, &quot;;&quot;, &quot;,&quot;, &quot;:&quot;), lexicon = &quot;bad punctuation&quot;, stringsAsFactors = F), by = &quot;word&quot;) # Get rid of the Gutenberg header word_counts &lt;- word_counts[72:nrow(word_counts),] # Put it all back together constitution &lt;- paste0(word_counts$word, collapse = &quot; &quot;) All of the above should be familiar: it is the same code we used in both the previous two tutorials. Now, let’s look at some new code. The textmineR package has a super useful function - CreateTcm() - which produces a skip-gram matrix, just like we want. All we have to do is specify the texts we want to run the skip-gram model on, the size of the skip-gram window, and how many cpus we want to use on our computer. library(textmineR) skip_gram_constitution &lt;- CreateTcm(doc_vec = constitution, skipgram_window = 10, verbose = FALSE, cpus = 1) The result is a word-to-word matrix, where cells are weighted by how frequently two words appear in the same window. We can easily turn this into a network. 10.3 From data to networks To turn our raw network data into an actual network, we will first convert this skip-gram matrix to a similarity matrix, where words are given similar weights if they occur in similar contexts together. To do that, we’ll use the proxy package. First we install it. install.packages(&quot;proxy&quot;) Then, we can use the simil function to measure the similarity between words in terms of their co-occurrences with other words. We’ll use cosine distance here, but there are many other distance measures you could consider using. library(proxy) ## ## Attaching package: &#39;proxy&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## as.matrix ## The following objects are masked from &#39;package:stats&#39;: ## ## as.dist, dist ## The following object is masked from &#39;package:base&#39;: ## ## as.matrix simil_constitution &lt;- simil(as.matrix(skip_gram_constitution), method = &quot;cosine&quot;) Cool! Next, we can use igraph, a user-maintained package in R, created for the sole purpose of analyzing networks. Installing igraph gives us a bunch of new tools for graphing, analyzing and manipulating networks, that don’t come with base R. The first step then is to install igraph. As always, to install a new package, we use the install.packages() function. It takes a character argument that is the name of the package you wish to install. install.packages(&quot;igraph&quot;) Now that igraph is installed, we need to use the library() function to load it into R. library(&quot;igraph&quot;) ## ## Attaching package: &#39;igraph&#39; ## The following object is masked from &#39;package:text2vec&#39;: ## ## normalize ## The following objects are masked from &#39;package:dplyr&#39;: ## ## as_data_frame, groups, union ## The following objects are masked from &#39;package:purrr&#39;: ## ## compose, simplify ## The following object is masked from &#39;package:tidyr&#39;: ## ## crossing ## The following object is masked from &#39;package:tibble&#39;: ## ## as_data_frame ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union This will allow us to use all of igraph’s functions. To analyze networks, igraph uses an object class called: “igraph”. We therefore have to convert our skip-gram matrix into an igraph object. Specifically, for our skip-gram matrix, we can use the graph.adjacency() function to do so. We will set weighted = T so that edges are weighted by their skip-gram value (words that more frequently appear in the same context will have higher weights.) Before we do that, we should also set the diagonal of the matrix to 0, so that words aren’t tied to themselves. skip_gram_constitution &lt;- as.matrix(skip_gram_constitution) diag(skip_gram_constitution) &lt;- 0 constitution_network &lt;- graph.adjacency(skip_gram_constitution, weighted = T) 10.4 Exploring your network We finally have a network in R! So.. what next? Well we can read a summary of it by typing its name into R. constitution_network ## IGRAPH d512784 DNW- 723 11804 -- ## + attr: name (v/c), weight (e/n) ## + edges from d512784 (vertex names): ## [1] ability-&gt;affirm ability-&gt;army ability-&gt;commander ## [4] ability-&gt;defend ability-&gt;preserve ability-&gt;solemnly ## [7] ability-&gt;swear ability-&gt;chief ability-&gt;execute ## [10] ability-&gt;faithfully ability-&gt;protect ability-&gt;affirmation ## [13] ability-&gt;oath ability-&gt;constitution ability-&gt;office ## [16] ability-&gt;section ability-&gt;president ability-&gt;united ## [19] abr -&gt;attest abr -&gt;baldwin abr -&gt;butler ## [22] abr -&gt;cotesworth abr -&gt;jackson abr -&gt;pierce ## + ... omitted several edges The first line - which begins with IGRAPH DN - tells us constitution_network is an igraph object and a directed network (DN), with N nodes and E edges. The next line tells us some attributes of the nodes and edges network. At the moment, it only has the attribute “name” for the vertices (v/c). We can look at the values of the “name” attribute with the V()$ function. head(V(constitution_network)$name) ## [1] &quot;ability&quot; &quot;abr&quot; &quot;absence&quot; &quot;absent&quot; &quot;absolutely&quot; ## [6] &quot;accept&quot; Finally, the last part gives us a snapshot of the edges present in the network, most of which are omitted. We can visualize the network using the plot() function. plot(constitution_network) The default visualization is pretty ugly… In the next section, we will learn how to improve the aesthetics of our network visualizations. 10.5 The Basics of Visualization Let’s start by adjusting the basic visualization. The most basic things we can change are the sizes and colors of nodes. When your network is large, often the nodes will appear too large and clumped together. The argument vertex.size allows you to adjust node size (vertex is the graph-theory term for node!). plot(constitution_network, vertex.size = 2) This is the basic way you change the settings of the plot function in igraph - you put a comma next to the next to the network object, type the name of the setting you want to change, and set it to a new value. Here we set vertex.size to 10. When you don’t change any settings, R will automatically use the default settings. You can find them in the help section for that function (i.e. by typing ?plot.igraph, for example). All you have to do is remember the names of the various settings, or look them up at: http://igraph.org/r/ The nodes have an ugly light orange color… We can use vertex.color to change their color to something nicer. We can also remove the ugly black frames by changing the vertex.frame.color setting to NA. Useful Link You can find a list of all the named colors in R at http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf plot(constitution_network, vertex.size = 2, vertex.color = &quot;tomato&quot;) The labels are too large and blue. We can adjust label size with vertex.label.cex. We can adjust the color with vertex.label.color plot(constitution_network, vertex.size = 2, vertex.color = &quot;tomato&quot;, vertex.label.cex = .2, vertex.label.color = &quot;black&quot;, edge.curved = .1, edge.arrow.size = .3, edge.width = .2) Alternatively, if we want to get rid of the labels, we can just set vertex.label to NA. plot(constitution_network, vertex.size = 2, vertex.color = &quot;tomato&quot;, vertex.label = NA) Finally we can make the edges smaller and curved to give them a nicer aesthetic plot(constitution_network, vertex.size = 2, vertex.color = &quot;tomato&quot;, vertex.label.cex = .2, vertex.label.color = &quot;black&quot;, edge.curved = .1, edge.arrow.size = .1, edge.width = .2) But don’t go too crazy! If you set edge.curved to be greater than .1, it will start to look like spaghetti. plot(constitution_network, vertex.size = 2, vertex.color = &quot;tomato&quot;, vertex.label.cex = .2, vertex.label.color = &quot;black&quot;, edge.curved = 1, edge.arrow.size = .1, edge.width = .2) 10.6 Layouts igraph has set of layout functions which, when passed a network object, return an array of coordinates that can then used when plotting that network. These coordinates should be saved to a separate R object, which is then called within the plot function. They all have the format: layout DOT algorithm name. For example, layout.kamada.kawai() or layout.fruchterman.reingold() Kamada Kawai # first we run the layout function on our graph kamadaLayout &lt;- layout.kamada.kawai(constitution_network) # and then we change the default layout setting to equal the layout we generated above plot(constitution_network, layout = kamadaLayout, vertex.size = 2, vertex.color = &quot;tomato&quot;, vertex.label.cex = .2, vertex.label.color = &quot;black&quot;, edge.curved = .1, edge.arrow.size = .3, edge.width = .2) Frucherman-Reingold # first we run the layout function on our graph fruchtermanLayout &lt;- layout.fruchterman.reingold(constitution_network) # and then we change the default layout setting to equal the layout we generated above plot(constitution_network, layout = fruchtermanLayout, vertex.size = 2, vertex.color = &quot;tomato&quot;, vertex.label.cex = .2, vertex.label.color = &quot;black&quot;, edge.curved = .1, edge.arrow.size = .3, edge.width = .2) You can see ?layout_ for more options and details. 10.7 Group detection Finally, like we did in the Text as Data week, we can identify groups of words which tend to appear together and color nodes by their group affiliation. First, identify groups using the wakltrap clustering algorithm and extract their group memberships. groups &lt;- walktrap.community(constitution_network) group_memberships &lt;- membership(groups) Then color nodes by their membership and plot the result. plot(constitution_network, layout = fruchtermanLayout, vertex.size = 2, vertex.color = group_memberships, vertex.label.cex = .2, vertex.label.color = &quot;black&quot;, edge.curved = .1, edge.arrow.size = .3, edge.width = .2) No lab this week! Work on your projects! "],["working-with-weird-data-an-example.html", "11 Working with weird data, an example 11.1 Reading in PDF data", " 11 Working with weird data, an example By now, you have a wide range of skills when it comes to analyzing data. You can design your own surveys, analyze networks, text data, text networks, perform machine learning and linear regression. The goal of this week is to prepare you for some of the inevitable difficulties you will run into when starting your own data projects. There is so much data out there and a lot of it is messy or in formats that you can’t easily analyze. One common data format you will run into, especially in social histroy, is PDF. We use PDFs all the time for documents ranging from articles to contracts. PDFs can easily be opened one at a time using Adobe or Preview but are difficult to analyze systematically in R. Two common issues arise when working with PDFs. First, PDFs do not natively read into R. If you want PDF data, you’ll have to use packages like pdftools to read your PDF. But it gets even more complicated – older PDFs are often just simple images of texts, scanned into the computer. As a result, the data they hold is optical, and so you can’t simply read the text into R, even with pdftools at your disposal. As a result, we’ll have to learn how to perform Optimal Character Recognition (OCR), which uses machine learning to take images of texts and reads the text. 11.1 Reading in PDF data How can we read PDFs into R? If your PDF is already OCRed and has text associated with it, you can simply use the pdf_text() function from the pdftools packages in R to read in the text. For example, here, I read in a pdf from my own computer. First, install pdftools. install.packages(&quot;pdftools&quot;) Then load it in along with tidyverse library(pdftools) ## Using poppler version 20.12.1 library(tidyverse) Next, we can use the pdf_text function to read in the PDF. PDF &lt;- pdf_text(&quot;Data/Wimmer_Lewis_2010_Beyond and Below Racial Homophily.pdf&quot;) It is some pretty messy text data! We could use what we learned in previous weeks to clean it up. Overall though, it isn’t too bad. If you had a folder full of PDFs on your computer, you could loop through each file with a PDF extension, and read it into R. For example, here I do it for a folder of readings from social network analysis class. First, I list all the files in the folder. base_folder &lt;- &quot;Week 2 - Types of Networks&quot; files &lt;- list.files(base_folder) Then I check which files are pdfs which_pdf &lt;- grepl(&quot;pdf&quot;, files) I limit the files to the pdfs. files &lt;- files[which_pdf] Finally, I’ll read the pdfs into R using pdftools. week2_pdfs &lt;- sapply(paste0(base_folder, &quot;/&quot;, files), pdf_text) Everything looks good… except the second reading, “Threshold Models of Collective Behavior” by Mark Granovetter. It is completely empty! What is going on? It turns out that the PDF has not been OCRed. It was published in 1978, well before the large-scale digitization of journal articles. As a result, it is simply an image of the paper without any text associated with the words in the article. There is nothing for pdftools to grab! We’ll have to OCR it ourselves. We can use the tesseract engine to do that. Tesseract is a state-of-the-art OCR engine originally developed by HP and now sponsored by Google. It may be open source, but it is as accurate as many proprietary engines! Before we OCR Professor Granovetter’s paper, let’s work through a couple examples. First, we’ll OCR a test image. Go to http://jeroen.github.io/images/testocr.png to see it. Let’s install the tesseract package in R. install.packages(&quot;tesseract&quot;) Now we will load it in and select the english engine. library(tesseract) eng &lt;- tesseract(&quot;eng&quot;) Last, we can tell tesseract to OCR the image of the poem in the link above. text &lt;- tesseract::ocr(&quot;http://jeroen.github.io/images/testocr.png&quot;, engine = eng) cat(text) ## This is a lot of 12 point text to test the ## ocr code and see if it works on all types ## of file format. ## ## The quick brown dog jumped over the ## lazy fox. The quick brown dog jumped ## over the lazy fox. The quick brown dog ## jumped over the lazy fox. The quick ## brown dog jumped over the lazy fox. Pretty good! Sometimes you have to clean the image file a bit to make it work well with the OCR engine. We can use the magick package for that. It has functions to resize images, convert images to different color schemes, trim whitespace on images, and write the results to new images. install.packages(&quot;magick&quot;) Let’s load in magick library(magick) ## Linking to ImageMagick 6.9.12.3 ## Enabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp ## Disabled features: fftw, ghostscript, x11 We’ll load in the image. You can go to the link to see that it is a bit messy. input &lt;- image_read(&quot;https://jeroen.github.io/images/bowers.jpg&quot;) Now we can apply some of the magick functions to clean up the image. text &lt;- input %&gt;% image_resize(&quot;2000x&quot;) %&gt;% image_convert(type = &#39;Grayscale&#39;) %&gt;% image_trim(fuzz = 40) %&gt;% image_write(format = &#39;png&#39;, density = &#39;300x300&#39;) %&gt;% tesseract::ocr() How did we do? cat(text) ## The Life and Work of ## Fredson Bowers ## by ## G. THOMAS TANSELLE ## ## N EVERY FIELD OF ENDEAVOR THERE ARE A FEW FIGURES WHOSE ACCOM- ## plishment and influence cause them to be the symbols of their age; ## their careers and oeuvres become the touchstones by which the ## field is measured and its history told. In the related pursuits of ## analytical and descriptive bibliography, textual criticism, and scholarly ## editing, Fredson Bowers was such a figure, dominating the four decades ## after 1949, when his Principles of Bibliographical Description was pub- ## lished. By 1973 the period was already being called “the age of Bowers”: ## in that year Norman Sanders, writing the chapter on textual scholarship ## for Stanley Wells&#39;s Shakespeare: Select Bibliographies, gave this title to ## a section of his essay. For most people, it would be achievement enough ## to rise to such a position in a field as complex as Shakespearean textual ## studies; but Bowers played an equally important role in other areas. ## Editors of nineteenth-century American authors, for example, would ## also have to call the recent past “the age of Bowers,” as would the writers ## of descriptive bibliographies of authors and presses. His ubiquity in ## the broad field of bibliographical and textual study, his seemingly com- ## plete possession of it, distinguished him from his illustrious predeces- ## sors and made him the personification of bibliographical scholarship in ## ## his time. ## ## When in 1969 Bowers was awarded the Gold Medal of the Biblio- ## graphical Society in London, John Carter’s citation referred to the ## Principles as “majestic,” called Bowers’s current projects “formidable,” ## said that he had “imposed critical discipline” on the texts of several ## authors, described Studies in Bibliography as a “great and continuing ## achievement,” and included among his characteristics “uncompromising ## seriousness of purpose” and “professional intensity.” Bowers was not ## unaccustomed to such encomia, but he had also experienced his share of ## attacks: his scholarly positions were not universally popular, and he ## expressed them with an aggressiveness that almost seemed calculated to Not bad! Alright, let’s try reading in Granovetter’s piece. pngfile &lt;- pdftools::pdf_convert(&#39;Data/Granovetter_AJS_1978_Threshold_Models.pdf&#39;, dpi = 600) Wow.. It converts each page one by one! I guess we will have to analyze each page separately. Let’s just look at page 5. It looks pretty straight and nice. Let’s just try OCR on it. granovetter_page5 &lt;- tesseract::ocr(&quot;Data/Granovetter_AJS_1978_Threshold_Models_5.png&quot;) Cool! We would have to do this for every page and then put them all into a single vector. "]]
